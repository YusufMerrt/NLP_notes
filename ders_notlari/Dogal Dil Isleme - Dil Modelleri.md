# Dogal Dil Isleme - Dil Modelleri

**Toplam Sayfa:** 20
**Toplam GÃ¶rsel:** 30

---

## Sayfa 1

### GÃ¶rseller

![GÃ¶rsel 1](gorseller/Dogal Dil Isleme - Dil Modelleri_sayfa1_gorsel1.png)

*GÃ¶rsel 1: Dogal Dil Isleme - Dil Modelleri_sayfa1_gorsel1.png*

### Ä°Ã§erik

DOÄAL DÄ°L 
Ä°ÅLEMEYE GÄ°RÄ°Å
BAHAR DÃ–NEMÄ° - 2022-2023
BÄ°LGÄ°SAYAR MÃœHENDÄ°SLÄ°ÄÄ° BÃ–LÃœMÃœ
BURSA TEKNÄ°K ÃœNÄ°VERSÄ°TESÄ°
DR. HAYRI VOLKAN AGUN


---

## Sayfa 2

### GÃ¶rseller

![GÃ¶rsel 1](gorseller/Dogal Dil Isleme - Dil Modelleri_sayfa2_gorsel1.png)

*GÃ¶rsel 1: Dogal Dil Isleme - Dil Modelleri_sayfa2_gorsel1.png*

### Ä°Ã§erik

Ã–zet
â€¢Dil OlasÄ±lÄ±k Modelleri
â€¢EÅŸ dizimlilik
â€¢Yapay Sinir AÄŸlarÄ± Dil Modelleri


---

## Sayfa 3

### GÃ¶rseller

![GÃ¶rsel 1](gorseller/Dogal Dil Isleme - Dil Modelleri_sayfa3_gorsel1.png)

*GÃ¶rsel 1: Dogal Dil Isleme - Dil Modelleri_sayfa3_gorsel1.png*

### Ä°Ã§erik

Ã–zet
â‘CÃ¼mlenin kelime bÃ¶lÃ¼tlemesi yapÄ±lÄ±rken tÃ¼m kelimelerin boÅŸluk ile ayrÄ±ldÄ±ÄŸÄ±nÄ± kabul ediyoruz. 
Ã–rneÄŸin:
â‘â€œSan Francisco kÃ¶prÃ¼sÃ¼ altÄ±n kapÄ± kÃ¶prÃ¼sÃ¼ olarak adlandÄ±rÄ±lan bir asma kÃ¶prÃ¼dÃ¼r ve 1937 yÄ±lÄ±nda 
inÅŸa edilmiÅŸtir.â€
â‘BÃ¶lÃ¼tler: [San, Francisco, kÃ¶prÃ¼sÃ¼, altÄ±n, kapÄ±, kÃ¶prÃ¼sÃ¼, olarak, adlandÄ±rÄ±lan, bir asma, kÃ¶prÃ¼dÃ¼r, ve 
1937, yÄ±lÄ±nda, inÅŸa, edilmiÅŸtir, .]
â‘TÃ¼m bulunan bu kelimeler aslÄ±nda tam olarak ayrÄ±k deÄŸildir. 
â‘Ã–zel isimler: San Francisco
â‘Adlar: altÄ±n kapÄ± kÃ¶prÃ¼sÃ¼
â‘Eylemler: inÅŸa edilmiÅŸtir


---

## Sayfa 4

### GÃ¶rseller

![GÃ¶rsel 1](gorseller/Dogal Dil Isleme - Dil Modelleri_sayfa4_gorsel1.png)

*GÃ¶rsel 1: Dogal Dil Isleme - Dil Modelleri_sayfa4_gorsel1.png*

### Ä°Ã§erik

Zipf YasasÄ±
â‘Dildeki tÃ¼m kelimeler ve frekanslarÄ± gÃ¶z Ã¶nÃ¼ne alÄ±ndÄ±ÄŸÄ±nda bir dilde kullanÄ±lan toplam sÃ¶zcÃ¼k sayÄ±sÄ± 
sÃ¶zlÃ¼k ile ifade edilir.
â‘Ancak bu sÃ¶zlÃ¼k iÃ§erisinde her bir sÃ¶zcÃ¼ÄŸÃ¼n tÃ¼m dil kaynaklarÄ±nda kullanÄ±m sÄ±klÄ±ÄŸÄ± o sÃ¶zcÃ¼ÄŸÃ¼n 
sÄ±ralamasÄ±nÄ± belirler.
â‘Ã–rneÄŸin â€œbirâ€ sÃ¶zcÃ¼ÄŸÃ¼ bÃ¼yÃ¼k bir metin havuzunda 31215 kez geÃ§sin ve â€˜yaÅŸâ€™ sÃ¶zcÃ¼ÄŸÃ¼ ise 25000 kez 
geÃ§sin. Bu durumda bir sÃ¶zcÃ¼ÄŸÃ¼nÃ¼n frekansÄ± daha yÃ¼ksektir ve sÄ±ralamasÄ± daha baÅŸtadÄ±r. 
â‘Zipf kanuna gÃ¶re doÄŸada geÃ§en tÃ¼m rastsal sÄ±ralamalarda (Ã¶rneÄŸin ÅŸehir nÃ¼fus sÄ±ralamalarÄ±) kelime 
geÃ§me sÄ±klÄ±ÄŸÄ± ile sÄ±rasÄ± arasÄ±ndaki katsayÄ± sabittir. Ã–rneÄŸin:
â‘5. sÄ±rada geÃ§en bir kelimenin geÃ§me sÄ±klÄ±ÄŸÄ± ile 6. sÄ±rada geÃ§en kelimenin sÄ±klÄ±ÄŸÄ± arasÄ±ndaki oran bir 
birine Ã§ok yakÄ±ndÄ±r.
N
toplam = 90800, 
Kelime (ile) R = (2. sÄ±ra) = 3, F = (frekans) = 676, Zipf = 3 * 676/90800 = 0.022 
Kelime (ile) R = (6. sÄ±ra) = 6, F = (frekans) = 511, Zipf = 6 * 511/90800 = 0.033


---

## Sayfa 5

### GÃ¶rseller

![GÃ¶rsel 1](gorseller/Dogal Dil Isleme - Dil Modelleri_sayfa5_gorsel1.png)

*GÃ¶rsel 1: Dogal Dil Isleme - Dil Modelleri_sayfa5_gorsel1.png*

### Ä°Ã§erik

Zipf YasasÄ±
â‘Zipf yasasÄ± ile aÃ§Ä±klanmak istenen bir dilde kullanÄ±lan kelimeler ne olursa olsun o dildeki
kelimenin kullanÄ±m sÄ±klÄ±ÄŸÄ± ile sÄ±ralamasÄ± arasÄ±nda sabit bir oran vardÄ±r.
â‘Ä°nsanlar ve diÄŸer tÃ¼m canlÄ±lar doÄŸasÄ± gereÄŸi enerjiyi koruyarak hareket ederler. KonuÅŸma
ve anlamlaÅŸtÄ±rmada da bir kelimenin sÄ±k kullanÄ±lmasÄ± diÄŸerinin az kullanÄ±lmasÄ± dilin
geliÅŸiminde enerjin korunmasÄ± olarak aÃ§Ä±klanabilir.
â‘Bir dile bir anlamÄ± aÃ§Ä±klamak iÃ§in yeni
bir kelime eklendiÄŸinde bu kelimenin kullanÄ±m
sÄ±klÄ±ÄŸÄ± ve sÄ±rasÄ± doÄŸal olarak belirlenmiÅŸ olmaktadÄ±r.
â‘Dildeki sÃ¶zcÃ¼klere yeni sÃ¶zcÃ¼kler ekleyerek farklÄ± anlamlar aÃ§Ä±klanabilir ve dilin geliÅŸimi
ile bu sÃ¶zcÃ¼kler arasÄ±ndaki frekans sÄ±ralamalarÄ± deÄŸiÅŸebilir.


---

## Sayfa 6

### GÃ¶rseller

![GÃ¶rsel 1](gorseller/Dogal Dil Isleme - Dil Modelleri_sayfa6_gorsel1.png)

*GÃ¶rsel 1: Dogal Dil Isleme - Dil Modelleri_sayfa6_gorsel1.png*

### Ä°Ã§erik

CÃ¼mle olasÄ±lÄ±klarÄ± 
â‘Bir cÃ¼mlenin iÃ§erisinde barÄ±ndÄ±rdÄ±ÄŸÄ± her bir kelime iÃ§in belirlenen olasÄ±lÄ±k bÃ¼yÃ¼k bir metin 
havuzundaki toplam kelime sayÄ±sÄ± ve o kelimenin geÃ§me sayÄ±sÄ± kullanÄ±larak hesaplanÄ±r.
â‘ğ‘ƒ(ğ‘¤= ğ‘ğ‘–ğ‘Ÿ) = ğ‘“ğ‘Ÿğ‘’ğ‘˜ğ‘ğ‘›ğ‘ (ğ‘ğ‘–ğ‘Ÿ) / ğ‘¡ğ‘œğ‘ğ‘™ğ‘ğ‘š= 3180/10900
â‘CÃ¼mle olasÄ±lÄ±ÄŸÄ± ise her bir kelimenin cÃ¼mle iÃ§inde bulunduÄŸu konuma bakÄ±lmaksÄ±zÄ±n 
kelime olasÄ±lÄ±ÄŸÄ±nÄ±n Ã§arpÄ±mÄ±dÄ±r.    
â‘Ã–rneÄŸin: â€˜yÃ¼z olduâ€™ ile â€˜oldu yÃ¼zâ€™ olasÄ±lÄ±klarÄ± aynÄ±dÄ±r.
â‘ğ‘ƒğ‘¤1, ğ‘¤2 = ğ‘ƒğ‘¤1 âˆ—ğ‘ƒğ‘¤2
â‘CÃ¼mle olasÄ±lÄ±ÄŸÄ± neden gereklidir. Ã–rnek uygulamalar neler olabilir?


---

## Sayfa 7

### GÃ¶rseller

![GÃ¶rsel 1](gorseller/Dogal Dil Isleme - Dil Modelleri_sayfa7_gorsel1.png)

*GÃ¶rsel 1: Dogal Dil Isleme - Dil Modelleri_sayfa7_gorsel1.png*

![GÃ¶rsel 2](gorseller/Dogal Dil Isleme - Dil Modelleri_sayfa7_gorsel2.png)

*GÃ¶rsel 2: Dogal Dil Isleme - Dil Modelleri_sayfa7_gorsel2.png*

### Ä°Ã§erik

Entropi
â‘Entropi genel olarak enerjinin korunmasÄ± kanunu ile aÃ§Ä±klanmaktadÄ±r. 
â‘Benzer bir ÅŸekilde bir bilginin ifade edilmesinde gereken bit sayÄ±sÄ±nÄ±n hesabÄ±nda da kullanÄ±lmaktadÄ±r. 
â‘Entropy bir durumun gerÃ§ekleÅŸmesi yada gÃ¶zlemlenmesindeki olasÄ± etki olarak da ifade edilebilir.
â‘Ã–rneÄŸin: bir AVMâ€™ye her gÃ¼n gelen arabalar sÄ±rasÄ±yla  sedan, sedan, hatcback, sedan, hatchback, .., sedan 
olsun.
â‘Bu arabalarÄ±n her birinin gelme olasÄ±lÄ±ÄŸÄ± p(x) olsun. TÃ¼m bir ay borunca
P(sedan) = sayÄ±sÄ±/toplam = 100/200 = 0.5
P(hatchback) = sayÄ±/toplam = 50/200 = 0.25
P(station) = sayÄ±/toplam = 25/200 = 0.0125
P(sport) = sayÄ±/toplam = 25/200 = 0.0125 
â‘Bu durumda bir gÃ¼n iÃ§in gelen araÃ§larÄ±n entropisi (log 2 tabanÄ±na gÃ¶re) :
â‘H(x) = Ïƒğ‘¥ğ‘ƒğ‘¥âˆ—log( 1/ğ‘ƒ(ğ‘¥))
H(x) = 0.50 * 1.0 + 0.25 * 2 + 2 x 0.125 * 3  = 1.5


---

## Sayfa 8

### GÃ¶rseller

![GÃ¶rsel 1](gorseller/Dogal Dil Isleme - Dil Modelleri_sayfa8_gorsel1.png)

*GÃ¶rsel 1: Dogal Dil Isleme - Dil Modelleri_sayfa8_gorsel1.png*

![GÃ¶rsel 2](gorseller/Dogal Dil Isleme - Dil Modelleri_sayfa8_gorsel2.png)

*GÃ¶rsel 2: Dogal Dil Isleme - Dil Modelleri_sayfa8_gorsel2.png*

### Ä°Ã§erik

â‘Ã–rneÄŸin: Yoldan geÃ§en her bir araba
eÅŸit olasÄ±lÄ±kla geÃ§miÅŸ olsaydÄ±.
â‘P(sedan) = P(station) = P(heÃ§bek) = P(spor)
= 0.25
â‘Yoldan geÃ§en arabalar sÄ±rasÄ±yla 0.75,
0.125, 0.0125, 0.0 olasÄ±lÄ±kla geÃ§seydi.
â‘Beklenen entropi birincide her zaman
daha yÃ¼ksektir. Neden?
â‘Entropi beklenen durumlarÄ±n Ã§eÅŸitliliÄŸini
fazla olmasÄ±dÄ±r.
AÅŸaÄŸÄ±daki ÅŸekilde bu
entropi gÃ¶zlemlenmektedir.
Entropi
2 durum iÃ§in en yÃ¼ksek 
Entropy 
olasÄ±lÄ±klarÄ±n eÅŸit olduÄŸu 
0.5 
ise gerÃ§ekleÅŸir.


---

## Sayfa 9

### GÃ¶rseller

![GÃ¶rsel 1](gorseller/Dogal Dil Isleme - Dil Modelleri_sayfa9_gorsel1.png)

*GÃ¶rsel 1: Dogal Dil Isleme - Dil Modelleri_sayfa9_gorsel1.png*

![GÃ¶rsel 2](gorseller/Dogal Dil Isleme - Dil Modelleri_sayfa9_gorsel2.png)

*GÃ¶rsel 2: Dogal Dil Isleme - Dil Modelleri_sayfa9_gorsel2.png*

### Ä°Ã§erik

â€¢
Entropy ile bir dilin tÃ¼m kelimelerini kullanarak ne kadar bilgi iÃ§erdiÄŸini hesaplayabilirdik.
â€¢
Ancak bunun iÃ§in Ã§ok bÃ¼yÃ¼k bir metin kÃ¼mesine sahip olmamÄ±z gerekirdi. Peki Ã§ok daha
az metin kullanarak bir dilin olasÄ±ksal olarak ne Ã¼rettiÄŸini nasÄ±l hesaplayabiliriz.
â€¢
Bunun iÃ§in tÃ¼m olasÄ±lÄ±ksal durumlarÄ± yerine Ã¶rneÄŸin tÃ¼m kelimelerin gerÃ§ek olasÄ±lÄ±klarÄ±
yerine kendimiz bir model oluÅŸturup bu modelin Ã¼rettiÄŸi olasÄ±lÄ±klarÄ± kullanÄ±rsak bu durumda
gerÃ§ek dÃ¼nyaya bir yakÄ±nsama yapabiliriz.
â€¢
Modelin bilgi oluÅŸturma kapasitesini Ã¶lÃ§mek iÃ§in Perplexity kullanÄ±labilir.
Perplexity


---

## Sayfa 10

### GÃ¶rseller

![GÃ¶rsel 1](gorseller/Dogal Dil Isleme - Dil Modelleri_sayfa10_gorsel1.png)

*GÃ¶rsel 1: Dogal Dil Isleme - Dil Modelleri_sayfa10_gorsel1.png*

![GÃ¶rsel 2](gorseller/Dogal Dil Isleme - Dil Modelleri_sayfa10_gorsel2.png)

*GÃ¶rsel 2: Dogal Dil Isleme - Dil Modelleri_sayfa10_gorsel2.png*

### Ä°Ã§erik

â€¢ Perplexity yerine cross entropy kullanarak bir modelin ne kadar iyi tahmin 
yaptÄ±ÄŸÄ±nÄ± tespit etmede kullanÄ±lÄ±r.
Perplexity ks Cross Entropy
Model olasÄ±lÄ±ÄŸÄ±
GerÃ§ek olasÄ±lÄ±k


---

## Sayfa 11

### GÃ¶rseller

![GÃ¶rsel 1](gorseller/Dogal Dil Isleme - Dil Modelleri_sayfa11_gorsel1.png)

*GÃ¶rsel 1: Dogal Dil Isleme - Dil Modelleri_sayfa11_gorsel1.png*

### Ä°Ã§erik

Dil Modelleri
â‘Bir cÃ¼mle yada kelime torbasÄ± iÃ§indeki her bir kelimenin ayrÄ± ayrÄ± perplexity deÄŸeri
hesaplanabilir.
â‘Ancak ayrÄ±k hesaplamada farz edilen baÄŸÄ±msÄ±z Ã¶zdeÅŸ daÄŸÄ±lÄ±m (independent and identically
distributed â€“ i.i.d.) gerÃ§ek dÃ¼nya iÃ§in Ã§ok eksik bir yaklaÅŸÄ±mdÄ±r.
â‘GerÃ§ek dÃ¼nyada her bir kelimenin olasÄ±lÄ±ÄŸÄ± birbirini etkiler. Ã–rneÄŸin: spor kelimesinin geÃ§mesi
ile futbol kelimesinin geÃ§mesi birbirinden baÄŸÄ±msÄ±z deÄŸildir. Burada cÃ¼mlenin yada sÄ±ralÄ± kelime
dizisinin kullanÄ±lmasÄ± ile cÃ¼mledeki kelimelerin daÄŸÄ±lÄ±mlarÄ± farklÄ± oluÅŸur. Bu fark ile olasÄ± veya
olasÄ± olmayan durumlar belirlenir.
â‘ArdÄ±ÅŸÄ±k kelime dizileri iÃ§in Ã¶rneÄŸin â€œSavaÅŸ tazminatÄ± aldÄ±lar .â€
cÃ¼mlesi iÃ§in her bir kelime
yanÄ±ndaki kelime ile iliÅŸki kabul edilirse o zaman dil modelinde olasÄ±lÄ±k hesabÄ± aÅŸaÄŸÄ±daki gibi
yapÄ±lmaktadÄ±r.
â‘p(cÃ¼mle) = p(savaÅŸ | BASLANGIC) * p(tazminatÄ± | savaÅŸ) * p(aldÄ±lar | tazminatÄ±) * p(. | aldÄ±lar)


---

## Sayfa 12

### GÃ¶rseller

![GÃ¶rsel 1](gorseller/Dogal Dil Isleme - Dil Modelleri_sayfa12_gorsel1.png)

*GÃ¶rsel 1: Dogal Dil Isleme - Dil Modelleri_sayfa12_gorsel1.png*

![GÃ¶rsel 2](gorseller/Dogal Dil Isleme - Dil Modelleri_sayfa12_gorsel2.png)

*GÃ¶rsel 2: Dogal Dil Isleme - Dil Modelleri_sayfa12_gorsel2.png*

### Ä°Ã§erik

Dil Modelleri
â‘AÅŸaÄŸÄ±da bir kelimenin baÄŸlÄ± olasÄ±lÄ±k hesabÄ± bir Ã¶nceki tÃ¼m kelimeler ile olan koÅŸullu olasÄ±lÄ±k 
hesabÄ±na gÃ¶re yapÄ±lmaktadÄ±r.
â‘Bir kelimenin kendinden Ã¶nceki kelimelere gÃ¶re olan koÅŸullu olasÄ±lÄ±k hesabÄ± aÅŸaÄŸÄ±daki gibi 
yapÄ±lmaktadÄ±r.
â‘ğ‘ğ‘¤ğ‘–, â€¦ . , ğ‘¤ğ‘š= #(ğ‘¤ğ‘–, â€¦ . , ğ‘¤ğ‘š)/# ğ‘¤ğ‘–, â€¦ . , ğ‘¤ğ‘šâˆ’1
â‘Ã–rneÄŸin bir metin havuzunda savaÅŸ kelimesi 1011 kez, ve savaÅŸ yasasÄ± kelimesi ise 605 
kez, ve savaÅŸ tazminatÄ± birlikte 11 kez geÃ§miÅŸ olsun. Bu durumda 
â‘p(â€œsavaÅŸ tazminatÄ±â€)  = #(â€œsavaÅŸ tazminatÄ±â€) / #(â€œsavaÅŸâ€) = 11/1011 = 0.0108
â‘p(â€œsavaÅŸ yasasÄ±â€) = #(â€œsavaÅŸ yasasÄ±â€) / #(â€œsavaÅŸâ€) = 605 / 1011 = 0.5984


---

## Sayfa 13

### GÃ¶rseller

![GÃ¶rsel 1](gorseller/Dogal Dil Isleme - Dil Modelleri_sayfa13_gorsel1.png)

*GÃ¶rsel 1: Dogal Dil Isleme - Dil Modelleri_sayfa13_gorsel1.png*

### Ä°Ã§erik

Dil Modelleri
â‘Dil modelleri bir kelimeden sonra baÅŸka hangi kelimenin geleceÄŸini tahmin etmek iÃ§in de 
kullanÄ±labilirler. Bu Ã¶zellikle SMS, E-Posta, Microsoft Word, Google Document gibi yazÄ±m 
araÃ§larÄ±nda kelime tamamlama Ã¶zelliÄŸinde kullanÄ±lÄ±r.
â‘Dil modelleri yÃ¶nlÃ¼ sonlu yapÄ±da olup Bayes yaklaÅŸÄ±mÄ±nÄ± barÄ±ndÄ±rÄ±rlar. 
â‘Dil modelleri ayrÄ±ca yapay sinir aÄŸlarÄ± ile ifade edilebilirler kullanÄ±labilir.  


---

## Sayfa 14

### GÃ¶rseller

![GÃ¶rsel 1](gorseller/Dogal Dil Isleme - Dil Modelleri_sayfa14_gorsel1.png)

*GÃ¶rsel 1: Dogal Dil Isleme - Dil Modelleri_sayfa14_gorsel1.png*

![GÃ¶rsel 2](gorseller/Dogal Dil Isleme - Dil Modelleri_sayfa14_gorsel2.png)

*GÃ¶rsel 2: Dogal Dil Isleme - Dil Modelleri_sayfa14_gorsel2.png*

![GÃ¶rsel 3](gorseller/Dogal Dil Isleme - Dil Modelleri_sayfa14_gorsel3.png)

*GÃ¶rsel 3: Dogal Dil Isleme - Dil Modelleri_sayfa14_gorsel3.png*

### Ä°Ã§erik

Yapay Sinir AÄŸlarÄ±
Yapay sinir aÄŸlarÄ± ayrÄ±mcÄ± (discriminative) sÄ±nÄ±flandÄ±rÄ±cÄ±lardÄ±r. SÄ±nÄ±flandÄ±rmak iÃ§in lineer aÄŸÄ±rlÄ±k matrisi 
kullanÄ±rlar ve bu aÄŸÄ±rlÄ±k matrisi gradyan (gradient) kullanÄ±larak veri Ã¼zerinden eÄŸitilir.
Dil modellerinde eÄŸitim iÃ§in ne kullanÄ±lÄ±r. Bir sÄ±nÄ±f yada kategori bilgisi yoktur.  


---

## Sayfa 15

### GÃ¶rseller

![GÃ¶rsel 1](gorseller/Dogal Dil Isleme - Dil Modelleri_sayfa15_gorsel1.png)

*GÃ¶rsel 1: Dogal Dil Isleme - Dil Modelleri_sayfa15_gorsel1.png*

![GÃ¶rsel 2](gorseller/Dogal Dil Isleme - Dil Modelleri_sayfa15_gorsel2.png)

*GÃ¶rsel 2: Dogal Dil Isleme - Dil Modelleri_sayfa15_gorsel2.png*

![GÃ¶rsel 3](gorseller/Dogal Dil Isleme - Dil Modelleri_sayfa15_gorsel3.png)

*GÃ¶rsel 3: Dogal Dil Isleme - Dil Modelleri_sayfa15_gorsel3.png*

### Ä°Ã§erik

Yapay Sinir AÄŸÄ± â€“ Dil Modelleri


---

## Sayfa 16

### GÃ¶rseller

![GÃ¶rsel 1](gorseller/Dogal Dil Isleme - Dil Modelleri_sayfa16_gorsel1.png)

*GÃ¶rsel 1: Dogal Dil Isleme - Dil Modelleri_sayfa16_gorsel1.png*

### Ä°Ã§erik

EÅŸdizimlilik
â‘Mevcut dil analizlerinde kullanÄ±lan ardÄ±ÅŸÄ±k dil modellerinde Ã§oÄŸu zaman tÃ¼m 
kelimeler ayrÄ±k kabul edilir.
â‘Ã–rneÄŸin
â‘Ä°ngilizce iÃ§in New York, fast food, do a favor, take a holiday
â‘TÃ¼rkÃ§e iÃ§in zaman kaybÄ±, sÄ±k sÄ±k, olan biten, rekor kÄ±rmak, rast gelmek, Ä°stanbul boÄŸazÄ±, 
avrupa yakasÄ±,â€¦, 


---

## Sayfa 17

### GÃ¶rseller

![GÃ¶rsel 1](gorseller/Dogal Dil Isleme - Dil Modelleri_sayfa17_gorsel1.png)

*GÃ¶rsel 1: Dogal Dil Isleme - Dil Modelleri_sayfa17_gorsel1.png*

![GÃ¶rsel 2](gorseller/Dogal Dil Isleme - Dil Modelleri_sayfa17_gorsel2.png)

*GÃ¶rsel 2: Dogal Dil Isleme - Dil Modelleri_sayfa17_gorsel2.png*

### Ä°Ã§erik

Pointwise Mutual Information
â‘Belirli hipotezlerin olasÄ±lÄ±klarÄ±n tutarlÄ± olup
olmadÄ±ÄŸÄ±nÄ± test etmek iÃ§in kullanÄ±lÄ±r.
â‘Ã–rneÄŸin bir metin iÃ§inde geÃ§en kelimelerin bir
eÅŸ dizimlilik oluÅŸturduÄŸunu test etmek iÃ§in
kullanÄ±labilir.
â‘Ã–rneÄŸin
yandaki
tabloya
gÃ¶re
mutual
information
ve
Chi-square
hipotez
testi
deÄŸerleri verilmiÅŸtir. Burada house chambre
ve
house
communes
Ã§evirileri
iÃ§in
MI
hesaplamasÄ± yapÄ±lmÄ±ÅŸtÄ±r. DoÄŸru Ã§eviri house
champre Ã§evirisidir.
â€¢ ğ¼ğ‘¥, ğ‘¦= log2
ğ‘ƒğ‘¥ğ‘¦
ğ‘ƒğ‘¥âˆ—ğ‘ƒğ‘¦


---

## Sayfa 18

### GÃ¶rseller

![GÃ¶rsel 1](gorseller/Dogal Dil Isleme - Dil Modelleri_sayfa18_gorsel1.png)

*GÃ¶rsel 1: Dogal Dil Isleme - Dil Modelleri_sayfa18_gorsel1.png*

### Ä°Ã§erik

EÅŸdizimlilik
â‘Bir kelime grubunun birlikte sÄ±k geÃ§mesine gÃ¶re kelimeler eÅŸ-dizim olarak kabul edilebilir.
â‘Bir kelime grubu eÅŸ - dizim midir? NasÄ±l bulunabilir?
â‘Ã–rneÄŸin: â€œNew Yorkâ€ kelimesi New ve York kelimelerinden oluÅŸur. New ve York kelimeleri tek baÅŸlarÄ±na 
tÃ¼m metin havuzunda 541 ve 212 kez geÃ§miÅŸ olsun. 
â‘Bu durumda â€œNew Yorkâ€ birlikte 5 kez geÃ§iyorsa ve metin havuzun 1500 toplam kelime sayÄ±sÄ± var ise 
bu kelime ikilisi eÅŸ dizim midir?
â‘Genel olarak:
â‘p(New | York) 
â‘H0: P(New) * P(York) > P(New York)
â‘H0 null hipotezidir. Null hipotezi bir durumun rastgele oluÅŸtuÄŸu belirli bir Ã¶beÄŸin yada Ã¶zel bir baÄŸÄ±n olmadÄ±ÄŸÄ± durumu temsil eder. 
â‘YukarÄ±daki durumda null hipotezi New ve York kelimelerinin iliÅŸkisel bir baÄŸÄ±ntÄ± barÄ±ndÄ±rmadÄ±ÄŸÄ±nÄ± gÃ¶sterir.  Bu 
durumda New ve York kelimeleri birbirinden baÄŸÄ±msÄ±zdÄ±r. Birlikte bir eÅŸ dizimi temsil etmezler.


---

## Sayfa 19

### GÃ¶rseller

![GÃ¶rsel 1](gorseller/Dogal Dil Isleme - Dil Modelleri_sayfa19_gorsel1.png)

*GÃ¶rsel 1: Dogal Dil Isleme - Dil Modelleri_sayfa19_gorsel1.png*

### Ä°Ã§erik

EÅŸdizimlilik
â‘p(â€œNew Yorkâ€) = 5/1500 = 0.003
â‘p(â€œNewâ€) = 541 / 1500 = 0.36
â‘p(â€œYorkâ€) = 212 / 1500 = 0.14
â‘p(â€œNew Yorkâ€) < p(â€œNewâ€) * p(â€œYorkâ€) â” 0.003 < 0.05
â‘Bu durumda `null hipotezi` geÃ§erli olur.  


---

## Sayfa 20

### GÃ¶rseller

![GÃ¶rsel 1](gorseller/Dogal Dil Isleme - Dil Modelleri_sayfa20_gorsel1.png)

*GÃ¶rsel 1: Dogal Dil Isleme - Dil Modelleri_sayfa20_gorsel1.png*

### Ä°Ã§erik

Interpolasyon â€“ Seyrek geÃ§me
â‘Bazen hesaplamak istediÄŸimiz olasÄ±lÄ±klar elimizdeki veride olmayabilir. Ã–rneÄŸin zamazingolar
kelimesi elimizdeki metinde geÃ§memiÅŸ olabilir. Bu durumda bu kelime ile Ã¶bek oluÅŸturacak
kelimelerde 0 olasÄ±lÄ±k maduru olacaklardÄ±r. Bunu engellemek iÃ§in interpolasyondan faydalanÄ±lÄ±r.
â‘P(wn|wn-2,wn-1) = Î»1P(wn) + Î»2P(wn| wn-1) + Î»2P(wn| wn-1,wn-2)
â‘Yukaridaki hesaplamada lambda Î» ifadesi pozif bir katsayÄ±r. Bu durumda zamazingo kelimesi 
wn-2 ise sadece bir terim sÄ±fÄ±r olacaktÄ±r. DiÄŸer terimlerle hesaplamaya devam edilebilir.


---

