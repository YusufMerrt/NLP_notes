
--- SAYFA 1 ---
DOĞAL DİL
İŞLEMEYE GİRİŞ
BAHAR DÖNEMİ - 2022-2023
BİLGİSAYAR MÜHENDİSLİĞİ BÖLÜMÜ
BURSA TEKNİK ÜNİVERSİTESİ
DR. HAYRI VOLKAN AGUN
==================================================

--- SAYFA 2 ---
Özet
•Dil Olasılık Modelleri
•Eş dizimlilik
•Yapay Sinir Ağları Dil Modelleri
==================================================

--- SAYFA 3 ---
Özet
❑ Cümlenin kelime bölütlemesi yapılırken tüm kelimelerin boşluk ile ayrıldığını kabul ediyoruz.
Örneğin:
❑ “San Francisco köprüsü altın kapı köprüsü olarak adlandırılan bir asma köprüdür ve 1937 yılında
inşa edilmiştir.”
❑ Bölütler: [San, Francisco, köprüsü, altın, kapı, köprüsü, olarak, adlandırılan, bir asma, köprüdür, ve
1937, yılında, inşa, edilmiştir, .]
❑ Tüm bulunan bu kelimeler aslında tam olarak ayrık değildir.
❑ Özel isimler: San Francisco
❑ Adlar: altın kapı köprüsü
❑ Eylemler: inşa edilmiştir
==================================================

--- SAYFA 4 ---
Zipf Yasası
❑ Dildeki tüm kelimeler ve frekansları göz önüne alındığında bir dilde kullanılan toplam sözcük sayısı
sözlük ile ifade edilir.
❑ Ancak bu sözlük içerisinde her bir sözcüğün tüm dil kaynaklarında kullanım sıklığı o sözcüğün
sıralamasını belirler.
❑ Örneğin “bir” sözcüğü büyük bir metin havuzunda 31215 kez geçsin ve ‘yaş’ sözcüğü ise 25000 kez
geçsin. Bu durumda bir sözcüğünün frekansı daha yüksektir ve sıralaması daha baştadır.
❑ Zipf kanuna göre doğada geçen tüm rastsal sıralamalarda (örneğin şehir nüfus sıralamaları) kelime
geçme sıklığı ile sırası arasındaki katsayı sabittir. Örneğin:
❑ 5. sırada geçen bir kelimenin geçme sıklığı ile 6. sırada geçen kelimenin sıklığı arasındaki oran bir
birine çok yakındır.
N toplam = 90800,
Kelime (ile) R = (2. sıra) = 3, F = (frekans) = 676, Zipf = 3 * 676/90800 = 0.022
Kelime (ile) R = (6. sıra) = 6, F = (frekans) = 511, Zipf = 6 * 511/90800 = 0.033
==================================================

--- SAYFA 5 ---
Zipf Yasası
❑ Zipf yasası ile açıklanmak istenen bir dilde kullanılan kelimeler ne olursa olsun o dildeki
kelimenin kullanım sıklığı ile sıralaması arasında sabit bir oran vardır.
❑ İnsanlar ve diğer tüm canlılar doğası gereği enerjiyi koruyarak hareket ederler. Konuşma
ve anlamlaştırmada da bir kelimenin sık kullanılması diğerinin az kullanılması dilin
gelişiminde enerjin korunması olarak açıklanabilir.
❑ Bir dile bir anlamı açıklamak için yeni bir kelime eklendiğinde bu kelimenin kullanım
sıklığı ve sırası doğal olarak belirlenmiş olmaktadır.
❑ Dildeki sözcüklere yeni sözcükler ekleyerek farklı anlamlar açıklanabilir ve dilin gelişimi
ile bu sözcükler arasındaki frekans sıralamaları değişebilir.
==================================================

--- SAYFA 6 ---
Cümle olasılıkları
❑ Bir cümlenin içerisinde barındırdığı her bir kelime için belirlenen olasılık büyük bir metin
havuzundaki toplam kelime sayısı ve o kelimenin geçme sayısı kullanılarak hesaplanır.
❑ 𝑃(𝑤 = 𝑏𝑖𝑟) = 𝑓𝑟𝑒𝑘𝑎𝑛𝑠(𝑏𝑖𝑟) / 𝑡𝑜𝑝𝑙𝑎𝑚 = 3180/10900
❑ Cümle olasılığı ise her bir kelimenin cümle içinde bulunduğu konuma bakılmaksızın
kelime olasılığının çarpımıdır.
❑ Örneğin: ‘yüz oldu’ ile ‘oldu yüz’ olasılıkları aynıdır.
❑ 𝑃 𝑤 , 𝑤 = 𝑃 𝑤 ∗ 𝑃 𝑤
1 2 1 2
❑ Cümle olasılığı neden gereklidir. Örnek uygulamalar neler olabilir?
==================================================

--- SAYFA 7 ---
Entropi
❑ Entropi genel olarak enerjinin korunması kanunu ile açıklanmaktadır.
❑ Benzer bir şekilde bir bilginin ifade edilmesinde gereken bit sayısının hesabında da kullanılmaktadır.
❑ Entropy bir durumun gerçekleşmesi yada gözlemlenmesindeki olası etki olarak da ifade edilebilir.
❑ Örneğin: bir AVM’ye her gün gelen arabalar sırasıyla sedan, sedan, hatcback, sedan, hatchback, .., sedan
olsun.
❑ Bu arabaların her birinin gelme olasılığı p(x) olsun. Tüm bir ay borunca
P(sedan) = sayısı/toplam = 100/200 = 0.5
P(hatchback) = sayı/toplam = 50/200 = 0.25
P(station) = sayı/toplam = 25/200 = 0.0125
P(sport) = sayı/toplam = 25/200 = 0.0125
❑ Bu durumda bir gün için gelen araçların entropisi (log 2 tabanına göre) :
❑ H(x) =σ 𝑃 𝑥 ∗ log(1/𝑃(𝑥))
𝑥
H(x) = 0.50 * 1.0 + 0.25 * 2 + 2 x 0.125 * 3 = 1.5
==================================================

--- SAYFA 8 ---
Entropi
❑ Örneğin: Yoldan geçen her bir araba
eşit olasılıkla geçmiş olsaydı.
❑ P(sedan) = P(station) = P(heçbek) = P(spor)
= 0.25
❑ Yoldan geçen arabalar sırasıyla 0.75,
0.125, 0.0125, 0.0 olasılıkla geçseydi.
❑ Beklenen entropi birincide her zaman
daha yüksektir. Neden?
2 durum için en yüksek
Entropy
❑ Entropi beklenen durumların çeşitliliğini
olasılıkların eşit olduğu
fazla olmasıdır. Aşağıdaki şekilde bu
0.5
entropi gözlemlenmektedir.
ise gerçekleşir.
==================================================

--- SAYFA 9 ---
Perplexity
• Entropy ile bir dilin tüm kelimelerini kullanarak ne kadar bilgi içerdiğini hesaplayabilirdik.
• Ancak bunun için çok büyük bir metin kümesine sahip olmamız gerekirdi. Peki çok daha
az metin kullanarak bir dilin olasıksal olarak ne ürettiğini nasıl hesaplayabiliriz.
• Bunun için tüm olasılıksal durumları yerine örneğin tüm kelimelerin gerçek olasılıkları
yerine kendimiz bir model oluşturup bu modelin ürettiği olasılıkları kullanırsak bu durumda
gerçek dünyaya bir yakınsama yapabiliriz.
• Modelin bilgi oluşturma kapasitesini ölçmek için Perplexity kullanılabilir.
==================================================

--- SAYFA 10 ---
Perplexity ks Cross Entropy
• Perplexity yerine cross entropy kullanarak bir modelin ne kadar iyi tahmin
yaptığını tespit etmede kullanılır.
Model olasılığı
Gerçek olasılık
==================================================

--- SAYFA 11 ---
Dil Modelleri
❑ Bir cümle yada kelime torbası içindeki her bir kelimenin ayrı ayrı perplexity değeri
hesaplanabilir.
❑ Ancak ayrık hesaplamada farz edilen bağımsız özdeş dağılım (independent and identically
distributed – i.i.d.) gerçek dünya için çok eksik bir yaklaşımdır.
❑ Gerçek dünyada her bir kelimenin olasılığı birbirini etkiler. Örneğin: spor kelimesinin geçmesi
ile futbol kelimesinin geçmesi birbirinden bağımsız değildir. Burada cümlenin yada sıralı kelime
dizisinin kullanılması ile cümledeki kelimelerin dağılımları farklı oluşur. Bu fark ile olası veya
olası olmayan durumlar belirlenir.
❑ Ardışık kelime dizileri için örneğin “Savaş tazminatı aldılar .” cümlesi için her bir kelime
yanındaki kelime ile ilişki kabul edilirse o zaman dil modelinde olasılık hesabı aşağıdaki gibi
yapılmaktadır.
❑ p(cümle) = p(savaş | BASLANGIC) * p(tazminatı | savaş) * p(aldılar | tazminatı) * p(. | aldılar)
==================================================

--- SAYFA 12 ---
Dil Modelleri
❑ Aşağıda bir kelimenin bağlı olasılık hesabı bir önceki tüm kelimeler ile olan koşullu olasılık
hesabına göre yapılmaktadır.
❑ Bir kelimenin kendinden önceki kelimelere göre olan koşullu olasılık hesabı aşağıdaki gibi
yapılmaktadır.
❑ 𝑝 𝑤 , … ., 𝑤 = #(𝑤 , …. , 𝑤 )/# 𝑤 ,… . ,𝑤
𝑖 𝑚 𝑖 𝑚 𝑖 𝑚−1
❑ Örneğin bir metin havuzunda savaş kelimesi 1011 kez, ve savaş yasası kelimesi ise 605
kez, ve savaş tazminatı birlikte 11 kez geçmiş olsun. Bu durumda
❑ p(“savaş tazminatı”) = #(“savaş tazminatı”) / #(“savaş”) = 11/1011 = 0.0108
❑ p(“savaş yasası”) = #(“savaş yasası”) / #(“savaş”) = 605 / 1011 = 0.5984
==================================================

--- SAYFA 13 ---
Dil Modelleri
❑ Dil modelleri bir kelimeden sonra başka hangi kelimenin geleceğini tahmin etmek için de
kullanılabilirler. Bu özellikle SMS, E-Posta, Microsoft Word, Google Document gibi yazım
araçlarında kelime tamamlama özelliğinde kullanılır.
❑ Dil modelleri yönlü sonlu yapıda olup Bayes yaklaşımını barındırırlar.
❑ Dil modelleri ayrıca yapay sinir ağları ile ifade edilebilirler kullanılabilir.
==================================================

--- SAYFA 14 ---
Yapay Sinir Ağları
Yapay sinir ağları ayrımcı (discriminative) sınıflandırıcılardır. Sınıflandırmak için lineer ağırlık matrisi
kullanırlar ve bu ağırlık matrisi gradyan (gradient) kullanılarak veri üzerinden eğitilir.
Dil modellerinde eğitim için ne kullanılır. Bir sınıf yada kategori bilgisi yoktur.
==================================================

--- SAYFA 15 ---
Yapay Sinir Ağı – Dil Modelleri
==================================================

--- SAYFA 16 ---
Eşdizimlilik
❑ Mevcut dil analizlerinde kullanılan ardışık dil modellerinde çoğu zaman tüm
kelimeler ayrık kabul edilir.
❑ Örneğin
❑ İngilizce için New York, fast food, do a favor, take a holiday
❑ Türkçe için zaman kaybı, sık sık, olan biten, rekor kırmak, rast gelmek, İstanbul boğazı,
avrupa yakası,…,
==================================================

--- SAYFA 17 ---
Pointwise Mutual Information
❑Belirli hipotezlerin olasılıkların tutarlı olup
𝑃 𝑥 𝑦
• 𝐼 𝑥, 𝑦 = log
olmadığını test etmek için kullanılır.
2
𝑃 𝑥 ∗𝑃 𝑦
❑Örneğin bir metin içinde geçen kelimelerin bir
eş dizimlilik oluşturduğunu test etmek için
kullanılabilir.
❑ Örneğin yandaki tabloya göre mutual
information ve Chi-square hipotez testi
değerleri verilmiştir. Burada house chambre
ve house communes çevirileri için MI
hesaplaması yapılmıştır. Doğru çeviri house
champre çevirisidir.
==================================================

--- SAYFA 18 ---
Eşdizimlilik
❑Bir kelime grubunun birlikte sık geçmesine göre kelimeler eş-dizim olarak kabul edilebilir.
❑Bir kelime grubu eş - dizim midir? Nasıl bulunabilir?
❑Örneğin: “New York” kelimesi New ve York kelimelerinden oluşur. New ve York kelimeleri tek başlarına
tüm metin havuzunda 541 ve 212 kez geçmiş olsun.
❑Bu durumda “New York” birlikte 5 kez geçiyorsa ve metin havuzun 1500 toplam kelime sayısı var ise
bu kelime ikilisi eş dizim midir?
❑Genel olarak:
❑p(New | York)
❑H0: P(New) * P(York) > P(New York)
❑H0 null hipotezidir. Null hipotezi bir durumun rastgele oluştuğu belirli bir öbeğin yada özel bir bağın olmadığı durumu temsil eder.
❑Yukarıdaki durumda null hipotezi New ve York kelimelerinin ilişkisel bir bağıntı barındırmadığını gösterir. Bu
durumda New ve York kelimeleri birbirinden bağımsızdır. Birlikte bir eş dizimi temsil etmezler.
==================================================

--- SAYFA 19 ---
Eşdizimlilik
❑p(“New York”) = 5/1500 = 0.003
❑p(“New”) = 541 / 1500 = 0.36
❑p(“York”) = 212 / 1500 = 0.14
❑p(“New York”) < p(“New”) * p(“York”) ➔ 0.003 < 0.05
❑Bu durumda `null hipotezi` geçerli olur.
==================================================

--- SAYFA 20 ---
Interpolasyon – Seyrek geçme
❑Bazen hesaplamak istediğimiz olasılıklar elimizdeki veride olmayabilir. Örneğin zamazingolar
kelimesi elimizdeki metinde geçmemiş olabilir. Bu durumda bu kelime ile öbek oluşturacak
kelimelerde 0 olasılık maduru olacaklardır. Bunu engellemek için interpolasyondan faydalanılır.
❑P(w |w ,w ) = λ P(w ) + λ P(w | w ) + λ P(w | w ,w )
n n-2 n-1 1 n 2 n n-1 2 n n-1 n-2
❑ Yukaridaki hesaplamada lambda λ ifadesi pozif bir katsayır. Bu durumda zamazingo kelimesi
w ise sadece bir terim sıfır olacaktır. Diğer terimlerle hesaplamaya devam edilebilir.
n-2
==================================================
