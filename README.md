# ğŸ“ DOÄAL DÄ°L Ä°ÅLEME - DETAYLI KONU ANLATIMI

## ğŸ¯ 1. MARKOV MODELÄ° - DETAYLI ANALÄ°Z

### ğŸ“– **Teorik Temeller**

#### **Tarihsel GeliÅŸim:**
Markov modelleri, **Andrey Markov** tarafÄ±ndan 1906'da geliÅŸtirildi. Ä°lk olarak Rus ÅŸiirindeki sesli harf dizilerini analiz etmek iÃ§in kullanÄ±ldÄ±.

#### **Matematiksel Temel:**
Markov Ã¶zelliÄŸi, bir **stokastik sÃ¼reÃ§**'in gelecekteki durumunun **yalnÄ±zca mevcut duruma** baÄŸlÄ± olduÄŸunu, geÃ§miÅŸ durumlarÄ±n etkisinin olmadÄ±ÄŸÄ±nÄ± belirtir.

**Formal TanÄ±m:**
```
P(Xâ‚™â‚Šâ‚ = x | Xâ‚ = xâ‚, Xâ‚‚ = xâ‚‚, ..., Xâ‚™ = xâ‚™) = P(Xâ‚™â‚Šâ‚ = x | Xâ‚™ = xâ‚™)
```

### ğŸ”¢ **Dil Modellemede Markov Zinciri**

#### **N-Gram Modelleri:**
- **Unigram:** P(wâ‚, wâ‚‚, ..., wâ‚™) = P(wâ‚) Ã— P(wâ‚‚) Ã— ... Ã— P(wâ‚™)
- **Bigram:** P(wâ‚, wâ‚‚, ..., wâ‚™) = P(wâ‚) Ã— P(wâ‚‚|wâ‚) Ã— P(wâ‚ƒ|wâ‚‚) Ã— ... Ã— P(wâ‚™|wâ‚™â‚‹â‚)
- **Trigram:** P(wâ‚, wâ‚‚, ..., wâ‚™) = P(wâ‚) Ã— P(wâ‚‚|wâ‚) Ã— P(wâ‚ƒ|wâ‚,wâ‚‚) Ã— ... Ã— P(wâ‚™|wâ‚™â‚‹â‚‚,wâ‚™â‚‹â‚)

#### **DetaylÄ± Ã–rnek - Bigram Model:**

**Ä°ngilizce Ã–rnek:**

**Verilen Kelime GeÃ§iÅŸ OlasÄ±lÄ±klarÄ±:**
```
P(BAÅLANGIÃ‡|savaÅŸ) = 0.1
P(savaÅŸ|tazminatÄ±) = 0.3
P(tazminatÄ±|aldÄ±lar) = 0.2
P(aldÄ±lar|.) = 0.8
P(.|SON) = 0.9
```

**CÃ¼mle:** "SavaÅŸ tazminatÄ± aldÄ±lar."

**AdÄ±m AdÄ±m Hesaplama:**
```
P(cÃ¼mle) = P(savaÅŸ|BAÅLANGIÃ‡) Ã— P(tazminatÄ±|savaÅŸ) Ã— P(aldÄ±lar|tazminatÄ±) Ã— P(.|aldÄ±lar) Ã— P(SON|.)

P(cÃ¼mle) = 0.1 Ã— 0.3 Ã— 0.2 Ã— 0.8 Ã— 0.9
P(cÃ¼mle) = 0.00432
```

**TÃ¼rkÃ§e Ã–rnek:**

**Verilen Kelime GeÃ§iÅŸ OlasÄ±lÄ±klarÄ±:**
```
P(BAÅLANGIÃ‡|bugÃ¼n) = 0.15
P(bugÃ¼n|hava) = 0.4
P(hava|Ã§ok) = 0.3
P(Ã§ok|gÃ¼zel) = 0.6
P(gÃ¼zel|.) = 0.7
P(.|SON) = 0.9
```

**CÃ¼mle:** "BugÃ¼n hava Ã§ok gÃ¼zel."

**AdÄ±m AdÄ±m Hesaplama:**
```
P(cÃ¼mle) = P(bugÃ¼n|BAÅLANGIÃ‡) Ã— P(hava|bugÃ¼n) Ã— P(Ã§ok|hava) Ã— P(gÃ¼zel|Ã§ok) Ã— P(.|gÃ¼zel) Ã— P(SON|.)

P(cÃ¼mle) = 0.15 Ã— 0.4 Ã— 0.3 Ã— 0.6 Ã— 0.7 Ã— 0.9
P(cÃ¼mle) = 0.006804
```

**KarÅŸÄ±laÅŸtÄ±rmalÄ± Analiz:**

| Ã–zellik | Ä°ngilizce | TÃ¼rkÃ§e |
|---------|-----------|--------|
| **CÃ¼mle** | "SavaÅŸ tazminatÄ± aldÄ±lar." | "BugÃ¼n hava Ã§ok gÃ¼zel." |
| **Uzunluk** | 4 kelime | 5 kelime |
| **OlasÄ±lÄ±k** | 0.00432 | 0.006804 |
| **Konu** | Tarihsel | GÃ¼nlÃ¼k |
| **KarmaÅŸÄ±klÄ±k** | Orta | DÃ¼ÅŸÃ¼k |

### ğŸ“Š **Markov Modelinin AvantajlarÄ± ve DezavantajlarÄ±**

#### âœ… **Avantajlar:**
- **Hesaplama KolaylÄ±ÄŸÄ±:** Sadece mevcut duruma bakÄ±lÄ±r
- **Bellek VerimliliÄŸi:** GeÃ§miÅŸ bilgiler saklanmaz
- **HÄ±zlÄ± EÄŸitim:** Az parametre ile eÄŸitilir

#### âŒ **Dezavantajlar:**
- **KÄ±sa HafÄ±za:** Uzun baÄŸÄ±mlÄ±lÄ±klarÄ± yakalayamaz
- **SÄ±fÄ±r OlasÄ±lÄ±k:** EÄŸitim verisinde gÃ¶rÃ¼lmeyen geÃ§iÅŸler
- **BaÄŸlam EksikliÄŸi:** Uzun mesafeli baÄŸlamlarÄ± unutur

### ğŸ”§ **Smoothing Teknikleri**

#### **Laplace Smoothing (Add-1):**
```
P(wáµ¢|wáµ¢â‚‹â‚) = (count(wáµ¢â‚‹â‚, wáµ¢) + 1) / (count(wáµ¢â‚‹â‚) + V)
```
V = kelime daÄŸarcÄ±ÄŸÄ± boyutu

#### **Interpolasyon:**
```
P(wáµ¢|wáµ¢â‚‹â‚‚, wáµ¢â‚‹â‚) = Î»â‚P(wáµ¢) + Î»â‚‚P(wáµ¢|wáµ¢â‚‹â‚) + Î»â‚ƒP(wáµ¢|wáµ¢â‚‹â‚‚, wáµ¢â‚‹â‚)
```

---

## ğŸ¯ 2. SHIFT-REDUCE ALGORÄ°TMASI - DETAYLI ANALÄ°Z

### ğŸ“– **Teorik Temeller**

#### **Tarihsel GeliÅŸim:**
Shift-Reduce algoritmasÄ±, **Donald Knuth** tarafÄ±ndan 1965'te geliÅŸtirilen **LR parsing** algoritmasÄ±nÄ±n temelidir. **Bottom-up parsing** yaklaÅŸÄ±mÄ±nÄ± kullanÄ±r.

#### **Matematiksel Temel:**
Shift-Reduce, **yÄ±ÄŸÄ±n (stack)** tabanlÄ± bir algoritmadÄ±r. CÃ¼mleyi **soldan saÄŸa** iÅŸler ve **en saÄŸdan tÃ¼retme (rightmost derivation)** yapar.

**Temel Ä°ÅŸlemler:**
1. **Shift:** Girdi kelimesini yÄ±ÄŸÄ±na ekle
2. **Reduce:** YÄ±ÄŸÄ±ndaki Ã¶ÄŸeleri gramer kuralÄ±na gÃ¶re birleÅŸtir
3. **Accept:** CÃ¼mle baÅŸarÄ±yla parse edildi
4. **Error:** Gramer hatasÄ± tespit edildi

### ğŸ”¢ **Algoritma AdÄ±mlarÄ± - DetaylÄ± AÃ§Ä±klama**

#### **AdÄ±m 1: YÄ±ÄŸÄ±n ve Girdi HazÄ±rlama**
```
YÄ±ÄŸÄ±n: [] (boÅŸ)
Girdi: [wâ‚, wâ‚‚, wâ‚ƒ, ..., wâ‚™]
```

#### **AdÄ±m 2: Ä°ÅŸlem DÃ¶ngÃ¼sÃ¼**
Her adÄ±mda ÅŸu kararlardan biri alÄ±nÄ±r:
- **Shift:** Girdiden kelime al, yÄ±ÄŸÄ±na ekle
- **Reduce:** YÄ±ÄŸÄ±ndaki Ã¶ÄŸeleri gramer kuralÄ±na gÃ¶re birleÅŸtir
- **Accept:** CÃ¼mle tamamlandÄ±
- **Error:** Gramer hatasÄ±

### ğŸ“ **DetaylÄ± Ã–rnek - AdÄ±m AdÄ±m**

#### **Ä°ngilizce Ã–rnek:**

**Gramer:**
```
S â†’ NP VP
NP â†’ Det N
VP â†’ V NP
Det â†’ the
N â†’ cat, dog
V â†’ chased
```

**CÃ¼mle:** "the cat chased the dog"

**AdÄ±m AdÄ±m Ä°ÅŸlem:**

| AdÄ±m | YÄ±ÄŸÄ±n | Girdi | Ä°ÅŸlem | AÃ§Ä±klama |
|------|-------|-------|-------|----------|
| 1 | [] | [the, cat, chased, the, dog] | Shift | "the" yÄ±ÄŸÄ±na ekle |
| 2 | [the] | [cat, chased, the, dog] | Shift | "cat" yÄ±ÄŸÄ±na ekle |
| 3 | [the, cat] | [chased, the, dog] | Reduce | Det + N â†’ NP |
| 4 | [NP] | [chased, the, dog] | Shift | "chased" yÄ±ÄŸÄ±na ekle |
| 5 | [NP, chased] | [the, dog] | Shift | "the" yÄ±ÄŸÄ±na ekle |
| 6 | [NP, chased, the] | [dog] | Shift | "dog" yÄ±ÄŸÄ±na ekle |
| 7 | [NP, chased, the, dog] | [] | Reduce | Det + N â†’ NP |
| 8 | [NP, chased, NP] | [] | Reduce | V + NP â†’ VP |
| 9 | [NP, VP] | [] | Reduce | NP + VP â†’ S |
| 10 | [S] | [] | Accept | CÃ¼mle tamamlandÄ± |

#### **TÃ¼rkÃ§e Ã–rnek:**

**Gramer:**
```
S â†’ NP VP
NP â†’ Det N
VP â†’ V NP
Det â†’ bu, ÅŸu, o
N â†’ kedi, kÃ¶pek, kuÅŸ
V â†’ kovaladÄ±, gÃ¶rdÃ¼
```

**CÃ¼mle:** "bu kedi kÃ¶peÄŸi kovaladÄ±"

**AdÄ±m AdÄ±m Ä°ÅŸlem:**

| AdÄ±m | YÄ±ÄŸÄ±n | Girdi | Ä°ÅŸlem | AÃ§Ä±klama |
|------|-------|-------|-------|----------|
| 1 | [] | [bu, kedi, kÃ¶peÄŸi, kovaladÄ±] | Shift | "bu" yÄ±ÄŸÄ±na ekle |
| 2 | [bu] | [kedi, kÃ¶peÄŸi, kovaladÄ±] | Shift | "kedi" yÄ±ÄŸÄ±na ekle |
| 3 | [bu, kedi] | [kÃ¶peÄŸi, kovaladÄ±] | Reduce | Det + N â†’ NP |
| 4 | [NP] | [kÃ¶peÄŸi, kovaladÄ±] | Shift | "kÃ¶peÄŸi" yÄ±ÄŸÄ±na ekle |
| 5 | [NP, kÃ¶peÄŸi] | [kovaladÄ±] | Shift | "kovaladÄ±" yÄ±ÄŸÄ±na ekle |
| 6 | [NP, kÃ¶peÄŸi, kovaladÄ±] | [] | Reduce | N + V â†’ VP |
| 7 | [NP, VP] | [] | Reduce | NP + VP â†’ S |
| 8 | [S] | [] | Accept | CÃ¼mle tamamlandÄ± |

#### **KarÅŸÄ±laÅŸtÄ±rmalÄ± Analiz:**

| Ã–zellik | Ä°ngilizce | TÃ¼rkÃ§e |
|---------|-----------|--------|
| **CÃ¼mle** | "the cat chased the dog" | "bu kedi kÃ¶peÄŸi kovaladÄ±" |
| **Uzunluk** | 5 kelime | 4 kelime |
| **AdÄ±m SayÄ±sÄ±** | 10 adÄ±m | 8 adÄ±m |
| **YapÄ±** | Det N V Det N | Det N N V |
| **KarmaÅŸÄ±klÄ±k** | Orta | DÃ¼ÅŸÃ¼k |
| **Reduce Ä°ÅŸlemleri** | 3 adet | 2 adet |

### ğŸ“Š **Shift-Reduce AlgoritmasÄ±nÄ±n AvantajlarÄ± ve DezavantajlarÄ±**

#### âœ… **Avantajlar:**
- **HÄ±zlÄ± Ä°ÅŸlem:** O(n) zaman karmaÅŸÄ±klÄ±ÄŸÄ±
- **Bellek Verimli:** Sadece yÄ±ÄŸÄ±n boyutu kadar bellek
- **GerÃ§ek ZamanlÄ±:** Kelime kelime iÅŸleyebilir
- **Hata Tespiti:** Ä°lk hatada durur
- **Esnek:** Herhangi bir gramer tÃ¼rÃ¼ ile Ã§alÄ±ÅŸÄ±r

#### âŒ **Dezavantajlar:**
- **KarmaÅŸÄ±k Karar:** Shift mi Reduce mu yapÄ±lacaÄŸÄ±na karar vermek zor
- **Ambiguity:** Birden fazla parse aÄŸacÄ± olabilir
- **Gramer BaÄŸÄ±mlÄ±lÄ±ÄŸÄ±:** Gramer kalitesi Ã¶nemli
- **Hata Kurtarma:** Hata sonrasÄ± devam etmek zor

### ğŸ”§ **Karar MekanizmasÄ±**

#### **Shift-Reduce KararÄ±:**
Algoritma her adÄ±mda ÅŸu kriterlere bakar:
1. **YÄ±ÄŸÄ±ndaki son Ã¶ÄŸeler** gramer kuralÄ±na uyuyor mu?
2. **Girdideki sonraki kelime** hangi kuralÄ± bekliyor?
3. **Lookahead:** Bir sonraki kelimeye bakarak karar ver

#### **Ã–rnek Karar SÃ¼reci:**
```
YÄ±ÄŸÄ±n: [NP, chased]
Girdi: [the, dog]
Durum: chased (V) var, NP bekliyor
Karar: Shift (Ã§Ã¼nkÃ¼ NP oluÅŸturmak iÃ§in daha fazla kelime gerekli)
```

### âš¡ **Algoritma KarmaÅŸÄ±klÄ±ÄŸÄ±**

#### **Zaman KarmaÅŸÄ±klÄ±ÄŸÄ±:**
- **O(n)** - Her kelime bir kez iÅŸlenir
- **En kÃ¶tÃ¼ durum:** O(nÂ²) - Her adÄ±mda reduce

#### **Uzay KarmaÅŸÄ±klÄ±ÄŸÄ±:**
- **O(n)** - YÄ±ÄŸÄ±n boyutu cÃ¼mle uzunluÄŸu kadar

#### **Pratik Performans:**
```
n = 20 kelimelik cÃ¼mle iÃ§in:
- Zaman: ~20-40 iÅŸlem
- Uzay: ~20 kelime
```

---

## ğŸ¯ 3. CYK ALGORÄ°TMASI - DETAYLI ANALÄ°Z

### ğŸ“– **Teorik Temeller**

#### **Tarihsel GeliÅŸim:**
CYK (Cocke-Younger-Kasami) algoritmasÄ±, **John Cocke**, **Daniel Younger** ve **Tadao Kasami** tarafÄ±ndan 1960'larda baÄŸÄ±msÄ±z olarak geliÅŸtirildi.

#### **Matematiksel Temel:**
CYK, **Chomsky Normal Form** (CNF) gramerlerle Ã§alÄ±ÅŸan **dinamik programlama** algoritmasÄ±dÄ±r.

**CNF KurallarÄ±:**
1. A â†’ BC (iki terminal olmayan)
2. A â†’ a (tek terminal)
3. S â†’ Îµ (sadece baÅŸlangÄ±Ã§ sembolÃ¼ iÃ§in)

### ğŸ”¢ **Algoritma AdÄ±mlarÄ± - DetaylÄ± AÃ§Ä±klama**

#### **AdÄ±m 1: Grameri CNF'ye DÃ¶nÃ¼ÅŸtÃ¼r**
```
Orijinal Gramer:
S â†’ NP VP
NP â†’ Det N | N
VP â†’ V NP | V NP PP
PP â†’ P NP
Det â†’ the, a
N â†’ cat, dog, telescope
V â†’ saw, chased
P â†’ with

CNF DÃ¶nÃ¼ÅŸÃ¼mÃ¼:
S â†’ NP VP
NP â†’ Det N
NP â†’ N
VP â†’ V NP
VP â†’ V NP_PP
NP_PP â†’ NP PP
PP â†’ P NP
Det â†’ the
Det â†’ a
N â†’ cat
N â†’ dog
N â†’ telescope
V â†’ saw
V â†’ chased
P â†’ with
```

#### **AdÄ±m 2: Tablo OluÅŸtur**
nÃ—n matris oluÅŸtur (n = cÃ¼mle uzunluÄŸu)

#### **AdÄ±m 3: KÃ¶ÅŸegen Doldur (Tek Kelimeler)**
```
CÃ¼mle: "the cat chased the dog"
Uzunluk: 5

    1     2     3     4     5
1  Det   -     -     -     -
2   -    N     -     -     -
3   -    -     V     -     -
4   -    -     -    Det    -
5   -    -     -     -     N
```

#### **AdÄ±m 4: Ãœst KÃ¶ÅŸegenleri Doldur**

**2'li Gruplar (Uzunluk 2):**
```
[1,2]: Det + N = NP
[2,3]: N + V = (geÃ§ersiz)
[3,4]: V + Det = (geÃ§ersiz)
[4,5]: Det + N = NP

    1     2     3     4     5
1  Det   NP    -     -     -
2   -    N     -     -     -
3   -    -     V     -     -
4   -    -     -    Det    NP
5   -    -     -     -     N
```

**3'lÃ¼ Gruplar (Uzunluk 3):**
```
[1,3]: Det + N + V = (geÃ§ersiz)
[2,4]: N + V + Det = (geÃ§ersiz)
[3,5]: V + Det + N = VP

    1     2     3     4     5
1  Det   NP    -     -     -
2   -    N     -     -     -
3   -    -     V     -     -
4   -    -     -    Det    NP
5   -    -     -     -     N
```

**4'lÃ¼ Gruplar (Uzunluk 4):**
```
[1,4]: Det + N + V + Det = (geÃ§ersiz)
[2,5]: N + V + Det + N = (geÃ§ersiz)
```

**5'li Gruplar (Uzunluk 5):**
```
[1,5]: Det + N + V + Det + N = S

    1     2     3     4     5
1  Det   NP    -     -     S
2   -    N     -     -     -
3   -    -     V     -     -
4   -    -     -    Det    NP
5   -    -     -     -     N
```

### ğŸ“ **DetaylÄ± Ã–rnek - AdÄ±m AdÄ±m**

#### **Ä°ngilizce Ã–rnek:**

**Gramer:**
```
S â†’ NP VP
NP â†’ Det N
VP â†’ V NP
Det â†’ the
N â†’ cat, dog
V â†’ chased
```

**CÃ¼mle:** "the cat chased the dog"

**Tablo Doldurma SÃ¼reci:**

1. **KÃ¶ÅŸegen (Tek Kelimeler):**
   - [1,1]: "the" â†’ Det
   - [2,2]: "cat" â†’ N
   - [3,3]: "chased" â†’ V
   - [4,4]: "the" â†’ Det
   - [5,5]: "dog" â†’ N

2. **2'li Gruplar:**
   - [1,2]: Det + N â†’ NP
   - [4,5]: Det + N â†’ NP

3. **3'lÃ¼ Gruplar:**
   - [3,5]: V + NP â†’ VP

4. **5'li Grup:**
   - [1,5]: NP + VP â†’ S

**SonuÃ§:** S kÃ¶ÅŸesinde S varsa cÃ¼mle gramer tarafÄ±ndan Ã¼retilir.

#### **TÃ¼rkÃ§e Ã–rnek:**

**Gramer:**
```
S â†’ NP VP
NP â†’ Det N
VP â†’ V NP
Det â†’ bu, ÅŸu, o
N â†’ kedi, kÃ¶pek, kuÅŸ
V â†’ kovaladÄ±, gÃ¶rdÃ¼
```

**CÃ¼mle:** "bu kedi kÃ¶peÄŸi kovaladÄ±"

**Tablo Doldurma SÃ¼reci:**

1. **KÃ¶ÅŸegen (Tek Kelimeler):**
   - [1,1]: "bu" â†’ Det
   - [2,2]: "kedi" â†’ N
   - [3,3]: "kÃ¶peÄŸi" â†’ N
   - [4,4]: "kovaladÄ±" â†’ V

2. **2'li Gruplar:**
   - [1,2]: Det + N â†’ NP
   - [3,4]: N + V â†’ VP

3. **4'lÃ¼ Grup:**
   - [1,4]: NP + VP â†’ S

**SonuÃ§:** S kÃ¶ÅŸesinde S varsa cÃ¼mle gramer tarafÄ±ndan Ã¼retilir.

#### **KarÅŸÄ±laÅŸtÄ±rmalÄ± Tablo:**

| Ã–zellik | Ä°ngilizce | TÃ¼rkÃ§e |
|---------|-----------|--------|
| **CÃ¼mle** | "the cat chased the dog" | "bu kedi kÃ¶peÄŸi kovaladÄ±" |
| **Uzunluk** | 5 kelime | 4 kelime |
| **YapÄ±** | Det N V Det N | Det N N V |
| **KarmaÅŸÄ±klÄ±k** | 5Ã—5 tablo | 4Ã—4 tablo |
| **Ä°ÅŸlem SayÄ±sÄ±** | 10 adÄ±m | 8 adÄ±m |

### âš¡ **Algoritma KarmaÅŸÄ±klÄ±ÄŸÄ±**

#### **Zaman KarmaÅŸÄ±klÄ±ÄŸÄ±:**
- **O(nÂ³)** - ÃœÃ§ iÃ§ iÃ§e dÃ¶ngÃ¼
- **Uzay KarmaÅŸÄ±klÄ±ÄŸÄ±:** O(nÂ²) - nÃ—n tablo

#### **Pratik Uygulamalar:**
```
n = 10 kelimelik cÃ¼mle iÃ§in:
- Zaman: 10Â³ = 1000 iÅŸlem
- Uzay: 10Â² = 100 hÃ¼cre
```

---

## ğŸ¯ 4. SHIFT-REDUCE vs CYK - DETAYLI KARÅILAÅTIRMA

### ğŸ“Š **Temel KarÅŸÄ±laÅŸtÄ±rma Tablosu**

| Ã–zellik | Shift-Reduce | CYK |
|---------|--------------|-----|
| **YÃ¶ntem** | YÄ±ÄŸÄ±n tabanlÄ± | Tablo doldurma |
| **Parsing TÃ¼rÃ¼** | Bottom-up | Bottom-up |
| **Gramer TÃ¼rÃ¼** | Herhangi | CNF |
| **Zaman KarmaÅŸÄ±klÄ±ÄŸÄ±** | O(n) | O(nÂ³) |
| **Uzay KarmaÅŸÄ±klÄ±ÄŸÄ±** | O(n) | O(nÂ²) |
| **GerÃ§ek ZamanlÄ±** | âœ… Evet | âŒ HayÄ±r |
| **Hata Tespiti** | HÄ±zlÄ± | YavaÅŸ |
| **Ambiguity** | Ä°lk bulduÄŸunu alÄ±r | TÃ¼m parse aÄŸaÃ§larÄ±nÄ± bulur |
| **Uygulama** | Pratik | Teorik |
| **Bellek KullanÄ±mÄ±** | DÃ¼ÅŸÃ¼k | YÃ¼ksek |

### ğŸ”„ **DetaylÄ± KarÅŸÄ±laÅŸtÄ±rma**

#### **1. Ã‡alÄ±ÅŸma Prensibi:**

**Shift-Reduce:**
```
- Soldan saÄŸa iÅŸler
- YÄ±ÄŸÄ±n kullanÄ±r
- Her adÄ±mda karar verir
- Ä°lk hatada durur
```

**CYK:**
```
- TÃ¼m cÃ¼mleyi gÃ¶rÃ¼r
- Tablo doldurur
- Sonunda karar verir
- TÃ¼m olasÄ±lÄ±klarÄ± hesaplar
```

#### **2. Gramer Gereksinimleri:**

**Shift-Reduce:**
```
- Herhangi bir gramer tÃ¼rÃ¼
- DÃ¶nÃ¼ÅŸtÃ¼rme gerekmez
- Esnek yapÄ±
```

**CYK:**
```
- Sadece CNF gramerler
- DÃ¶nÃ¼ÅŸtÃ¼rme zorunlu
- KÄ±sÄ±tlÄ± yapÄ±
```

#### **3. Hata YÃ¶netimi:**

**Shift-Reduce:**
```
- Ä°lk hatada durur
- HÄ±zlÄ± hata tespiti
- Hata kurtarma zor
```

**CYK:**
```
- TÃ¼m tabloyu doldurur
- YavaÅŸ hata tespiti
- Hata analizi kolay
```

### ğŸ“ **Pratik Ã–rneklerle KarÅŸÄ±laÅŸtÄ±rma**

#### **Ã–rnek 1: "the cat chased the dog"**

**Shift-Reduce SÃ¼reci:**
```
AdÄ±m 1: [the] â† Shift
AdÄ±m 2: [the, cat] â† Shift  
AdÄ±m 3: [NP] â† Reduce (Det + N â†’ NP)
AdÄ±m 4: [NP, chased] â† Shift
AdÄ±m 5: [NP, chased, the] â† Shift
AdÄ±m 6: [NP, chased, the, dog] â† Shift
AdÄ±m 7: [NP, chased, NP] â† Reduce (Det + N â†’ NP)
AdÄ±m 8: [NP, VP] â† Reduce (V + NP â†’ VP)
AdÄ±m 9: [S] â† Reduce (NP + VP â†’ S)
AdÄ±m 10: Accept
```

**CYK SÃ¼reci:**
```
1. Tablo oluÅŸtur (5Ã—5)
2. KÃ¶ÅŸegen doldur: [Det, N, V, Det, N]
3. 2'li gruplar: [NP, -, -, NP]
4. 3'lÃ¼ gruplar: [-, -, VP, -]
5. 5'li grup: [S]
6. SonuÃ§: S kÃ¶ÅŸesinde S var â†’ Kabul
```

#### **Ã–rnek 2: Ambiguous CÃ¼mle**

**CÃ¼mle:** "I saw the man with the telescope"

**Shift-Reduce:**
```
- Ä°lk bulduÄŸu parse aÄŸacÄ±nÄ± kabul eder
- HÄ±zlÄ± karar verir
- DiÄŸer olasÄ±lÄ±klarÄ± gÃ¶rmez
```

**CYK:**
```
- TÃ¼m olasÄ± parse aÄŸaÃ§larÄ±nÄ± bulur
- Ambiguity'yi tespit eder
- Hangi yapÄ±nÄ±n daha olasÄ± olduÄŸunu hesaplar
```

### âš¡ **Performans KarÅŸÄ±laÅŸtÄ±rmasÄ±**

#### **Zaman KarÅŸÄ±laÅŸtÄ±rmasÄ±:**
```
CÃ¼mle UzunluÄŸu | Shift-Reduce | CYK
10 kelime      | ~10-20 iÅŸlem | ~1000 iÅŸlem
20 kelime      | ~20-40 iÅŸlem | ~8000 iÅŸlem
50 kelime      | ~50-100 iÅŸlem| ~125000 iÅŸlem
```

#### **Bellek KarÅŸÄ±laÅŸtÄ±rmasÄ±:**
```
CÃ¼mle UzunluÄŸu | Shift-Reduce | CYK
10 kelime      | ~10 kelime   | ~100 hÃ¼cre
20 kelime      | ~20 kelime   | ~400 hÃ¼cre
50 kelime      | ~50 kelime   | ~2500 hÃ¼cre
```

### ğŸ¯ **Hangi Durumda Hangi Algoritma?**

#### **Shift-Reduce Kullan:**
- **GerÃ§ek zamanlÄ±** uygulamalar
- **HÄ±zlÄ±** parsing gerektiÄŸinde
- **Bellek** kÄ±sÄ±tlÄ± olduÄŸunda
- **Pratik** uygulamalar
- **Hata tespiti** Ã¶nemli olduÄŸunda

#### **CYK Kullan:**
- **Teorik** analiz
- **Ambiguity** tespiti
- **TÃ¼m olasÄ±lÄ±klarÄ±** gÃ¶rmek istediÄŸinde
- **Gramer doÄŸrulama**
- **Akademik** Ã§alÄ±ÅŸmalar

### ğŸ“Š **Gramer DÃ¶nÃ¼ÅŸtÃ¼rme Ã–rneÄŸi**

#### **Normal Gramer â†’ CNF:**

**Orijinal Gramer:**
```
S â†’ NP VP
NP â†’ Det N | N
VP â†’ V NP | V NP PP
PP â†’ P NP
Det â†’ the, a
N â†’ cat, dog, telescope
V â†’ saw, chased
P â†’ with
```

**CNF DÃ¶nÃ¼ÅŸÃ¼mÃ¼:**
```
S â†’ NP VP
NP â†’ Det N
NP â†’ N
VP â†’ V NP
VP â†’ V NP_PP
NP_PP â†’ NP PP
PP â†’ P NP
Det â†’ the
Det â†’ a
N â†’ cat
N â†’ dog
N â†’ telescope
V â†’ saw
V â†’ chased
P â†’ with
```

**DÃ¶nÃ¼ÅŸtÃ¼rme KurallarÄ±:**
1. **A â†’ BCD** â†’ **A â†’ BE, E â†’ CD**
2. **A â†’ a|b** â†’ **A â†’ a, A â†’ b**
3. **Terminal olmayanlar** sadece 2 terminal olmayan Ã¼retebilir

---

## ğŸ¯ 5. EXPECTATION (BEKLENTÄ°) - DETAYLI ANALÄ°Z

### ğŸ“– **Teorik Temeller**

#### **Matematiksel TanÄ±m:**
Beklenti, bir rastgele deÄŸiÅŸkenin **ortalama deÄŸeri**'dir.

**Formal TanÄ±m:**
```
E[X] = âˆ‘ xáµ¢ Ã— P(xáµ¢)
```

#### **SÃ¼rekli DeÄŸiÅŸkenler Ä°Ã§in:**
```
E[X] = âˆ« x Ã— f(x) dx
```

### ğŸ”¢ **Markov Modelinde Beklenti**

#### **Durum GeÃ§iÅŸ Beklentisi:**
Bir durumdan diÄŸerine geÃ§iÅŸin beklenen sayÄ±sÄ±.

**FormÃ¼l:**
```
E[GeÃ§iÅŸ] = âˆ‘ P(geÃ§iÅŸáµ¢) Ã— deÄŸeráµ¢
```

#### **DetaylÄ± Ã–rnek - Hava Durumu Modeli:**

**GeÃ§iÅŸ OlasÄ±lÄ±klarÄ±:**
```
GÃ¼neÅŸli â†’ YaÄŸmurlu: 0.3
GÃ¼neÅŸli â†’ Bulutlu: 0.5
GÃ¼neÅŸli â†’ GÃ¼neÅŸli: 0.2
```

**Beklenti Hesaplama:**
```
E[GeÃ§iÅŸ] = 0.3 Ã— 1 + 0.5 Ã— 1 + 0.2 Ã— 1 = 1
```

**AÃ§Ä±klama:** Her geÃ§iÅŸ 1 adÄ±m sayÄ±ldÄ±ÄŸÄ± iÃ§in toplam beklenti 1'dir.

### ğŸ“Š **Dil Modellemede Beklenti**

#### **Kelime Beklentisi:**
Bir kelimenin bir cÃ¼mlede kaÃ§ kez geÃ§eceÄŸinin beklenen deÄŸeri.

**Ã–rnek:**
```
P("the") = 0.05
CÃ¼mle uzunluÄŸu = 20

E["the"] = 20 Ã— 0.05 = 1
```

#### **CÃ¼mle UzunluÄŸu Beklentisi:**
Bir dil modelinin Ã¼reteceÄŸi cÃ¼mlelerin ortalama uzunluÄŸu.

**FormÃ¼l:**
```
E[Uzunluk] = âˆ‘ i Ã— P(Uzunluk = i)
```

### ğŸ”§ **KoÅŸullu Beklenti**

#### **TanÄ±m:**
Bir olayÄ±n gerÃ§ekleÅŸmesi koÅŸuluyla baÅŸka bir olayÄ±n beklenen deÄŸeri.

**FormÃ¼l:**
```
E[X|Y] = âˆ‘ x Ã— P(X=x|Y)
```

#### **Markov Modelinde:**
```
E[SonrakiKelime|MevcutKelime] = âˆ‘ kelime Ã— P(SonrakiKelime=kelime|MevcutKelime)
```

### ğŸ“ **Pratik Uygulamalar**

#### **1. Metin Ãœretimi:**
```
Verilen: "BugÃ¼n hava"
Beklenti: E[SonrakiKelime] = 0.3Ã—"gÃ¼zel" + 0.4Ã—"kÃ¶tÃ¼" + 0.3Ã—"yaÄŸmurlu"
SonuÃ§: "kÃ¶tÃ¼" (en yÃ¼ksek olasÄ±lÄ±k)
```

#### **2. Model DeÄŸerlendirmesi:**
```
Test cÃ¼mlesi: "the cat sat on the mat"
Model tahminleri:
- P("cat"|"the") = 0.1
- P("sat"|"cat") = 0.05
- P("on"|"sat") = 0.2
- P("the"|"on") = 0.3
- P("mat"|"the") = 0.15

Beklenti: E[OlasÄ±lÄ±k] = 0.1 Ã— 0.05 Ã— 0.2 Ã— 0.3 Ã— 0.15 = 0.000045
```

### âš¡ **Beklenti Hesaplama Teknikleri**

#### **1. Monte Carlo SimÃ¼lasyonu:**
```
1. Ã‡ok sayÄ±da rastgele Ã¶rnek Ã¼ret
2. Her Ã¶rneÄŸin deÄŸerini hesapla
3. OrtalamasÄ±nÄ± al
```

#### **2. Dinamik Programlama:**
```
1. Alt problemleri Ã§Ã¶z
2. SonuÃ§larÄ± tabloda sakla
3. BÃ¼yÃ¼k problemi kÃ¼Ã§Ã¼k parÃ§alardan oluÅŸtur
```

### ğŸ“Š **Beklenti vs Varyans**

#### **Beklenti (E[X]):**
- Ortalama deÄŸer
- Merkezi eÄŸilim

#### **Varyans (Var[X]):**
```
Var[X] = E[(X - E[X])Â²] = E[XÂ²] - (E[X])Â²
```
- DeÄŸiÅŸkenlik Ã¶lÃ§Ã¼sÃ¼
- DaÄŸÄ±lÄ±mÄ±n geniÅŸliÄŸi

### ğŸ”„ **Markov Modelinde Varyans**

#### **Durum GeÃ§iÅŸ VaryansÄ±:**
```
Var[GeÃ§iÅŸ] = E[GeÃ§iÅŸÂ²] - (E[GeÃ§iÅŸ])Â²
```

**Ã–rnek:**
```
GeÃ§iÅŸ deÄŸerleri: [1, 1, 1] (her geÃ§iÅŸ 1 adÄ±m)
OlasÄ±lÄ±klar: [0.3, 0.5, 0.2]

E[GeÃ§iÅŸ] = 1
E[GeÃ§iÅŸÂ²] = 0.3Ã—1Â² + 0.5Ã—1Â² + 0.2Ã—1Â² = 1
Var[GeÃ§iÅŸ] = 1 - 1Â² = 0
```

---

## ğŸ¯ SINAVDA Ã‡IKABÄ°LECEK SORU TÄ°PLERÄ°

### ğŸ“ **Markov Modeli SorularÄ±:**

#### **Soru Tipi 1: CÃ¼mle OlasÄ±lÄ±ÄŸÄ± Hesaplama**
```
Verilen: GeÃ§iÅŸ olasÄ±lÄ±klarÄ± ve bir cÃ¼mle
Ä°stenen: CÃ¼mlenin olasÄ±lÄ±ÄŸÄ±nÄ± hesapla

Ã‡Ã¶zÃ¼m: P(wâ‚...wâ‚™) = P(wâ‚) Ã— âˆP(wáµ¢|wáµ¢â‚‹â‚)
```

#### **Soru Tipi 2: Beklenti Hesaplama**
```
Verilen: Durum geÃ§iÅŸ olasÄ±lÄ±klarÄ±
Ä°stenen: Beklenen geÃ§iÅŸ sayÄ±sÄ±nÄ± hesapla

Ã‡Ã¶zÃ¼m: E[X] = âˆ‘xáµ¢ Ã— P(xáµ¢)
```

### ğŸ“ **CYK AlgoritmasÄ± SorularÄ±:**

#### **Soru Tipi 1: Tablo Doldurma**
```
Verilen: CNF gramer ve cÃ¼mle
Ä°stenen: CYK tablosunu doldur

Ã‡Ã¶zÃ¼m: KÃ¶ÅŸegenden baÅŸla, Ã¼st kÃ¶ÅŸegenleri doldur
```

#### **Soru Tipi 2: Gramer DÃ¶nÃ¼ÅŸtÃ¼rme**
```
Verilen: Normal gramer
Ä°stenen: CNF'ye dÃ¶nÃ¼ÅŸtÃ¼r

Ã‡Ã¶zÃ¼m: A â†’ BCD â†’ A â†’ BE, E â†’ CD
```

### ğŸ“ **Shift-Reduce SorularÄ±:**

#### **Soru Tipi 1: AdÄ±m AdÄ±m Parsing**
```
Verilen: Gramer ve cÃ¼mle
Ä°stenen: Shift-Reduce adÄ±mlarÄ±nÄ± gÃ¶ster

Ã‡Ã¶zÃ¼m: YÄ±ÄŸÄ±n ve girdi durumlarÄ±nÄ± takip et
```

#### **Soru Tipi 2: Karar MekanizmasÄ±**
```
Verilen: YÄ±ÄŸÄ±n ve girdi durumu
Ä°stenen: Shift mi Reduce mu yapÄ±lacaÄŸÄ±na karar ver

Ã‡Ã¶zÃ¼m: Gramer kurallarÄ±na ve lookahead'e bak
```

### ğŸ“ **Expectation SorularÄ±:**

#### **Soru Tipi 1: KoÅŸullu Beklenti**
```
Verilen: KoÅŸullu olasÄ±lÄ±klar
Ä°stenen: E[X|Y] hesapla

Ã‡Ã¶zÃ¼m: E[X|Y] = âˆ‘x Ã— P(X=x|Y)
```

#### **Soru Tipi 2: Model DeÄŸerlendirmesi**
```
Verilen: Test verisi ve model tahminleri
Ä°stenen: Beklenen performansÄ± hesapla

Ã‡Ã¶zÃ¼m: E[Performans] = âˆ‘P(tahmin) Ã— deÄŸer
```

---

## ğŸ“ BAÅARI TAVSÄ°YELERÄ°

### âœ… **Son GÃ¼n Ã‡alÄ±ÅŸma PlanÄ±:**

1. **Markov Modeli (2 saat):**
   - FormÃ¼lleri ezberle
   - Ã–rnek problemler Ã§Ã¶z
   - Beklenti hesaplamalarÄ± yap

2. **CYK AlgoritmasÄ± (2 saat):**
   - Tablo doldurma pratiÄŸi
   - CNF dÃ¶nÃ¼ÅŸÃ¼mleri
   - Shift-Reduce karÅŸÄ±laÅŸtÄ±rmasÄ±

3. **Shift-Reduce (1 saat):**
   - AdÄ±m adÄ±m parsing
   - Karar mekanizmasÄ±
   - Pratik Ã¶rnekler

4. **Expectation (1 saat):**
   - KoÅŸullu beklenti hesaplamalarÄ±
   - Varyans formÃ¼lleri
   - Pratik uygulamalar

### ğŸ¯ **SÄ±nav Ä°puÃ§larÄ±:**

- **Zaman yÃ¶netimi:** CYK en Ã§ok zaman alÄ±r
- **Hesaplama kontrolÃ¼:** SonuÃ§larÄ± mantÄ±k kontrolÃ¼nden geÃ§ir
- **Birim kontrolÃ¼:** OlasÄ±lÄ±klar 0-1 arasÄ±nda olmalÄ±
- **Grafik Ã§iz:** CYK iÃ§in tablo Ã§iz
- **AdÄ±m takibi:** Shift-Reduce iÃ§in yÄ±ÄŸÄ±n durumunu takip et

### ğŸ“ **FormÃ¼l KartÄ±:**
```
Markov: P(wâ‚...wâ‚™) = P(wâ‚) Ã— âˆP(wáµ¢|wáµ¢â‚‹â‚)
CYK: O(nÂ³) zaman, O(nÂ²) uzay
Shift-Reduce: O(n) zaman, O(n) uzay
Expectation: E[X] = âˆ‘xáµ¢ Ã— P(xáµ¢)
KoÅŸullu: E[X|Y] = âˆ‘x Ã— P(X=x|Y)
Varyans: Var[X] = E[XÂ²] - (E[X])Â²
```

---

**ğŸ“ BAÅARILAR! Final sÄ±navÄ±nda baÅŸarÄ±lar dilerim! ğŸ“**

*Bu dÃ¶kÃ¼man, CYK, Markov, Shift-Reduce ve Expectation konularÄ±na Ã¶zel olarak detaylandÄ±rÄ±lmÄ±ÅŸtÄ±r.*

### ğŸ“ **Pratik Ã–rnek - Spam Tespiti**

**Verilen:**
- 1000 e-posta test edildi
- 200 spam, 800 normal
- SonuÃ§lar:
  - 150 spam doÄŸru tespit edildi
  - 50 spam yanlÄ±ÅŸ tespit edildi (normal sanÄ±ldÄ±)
  - 100 normal yanlÄ±ÅŸ tespit edildi (spam sanÄ±ldÄ±)
  - 700 normal doÄŸru tespit edildi

**KarmaÅŸÄ±klÄ±k Matrisi:**
```
                Tahmin
GerÃ§ek    Spam    Normal
Spam      150     50
Normal    100     700
```

**Hesaplamalar:**
- TP = 150 (Spam doÄŸru tespit)
- FP = 100 (Normal yanlÄ±ÅŸ spam)
- TN = 700 (Normal doÄŸru tespit)
- FN = 50 (Spam yanlÄ±ÅŸ normal)

---

### ğŸ“ **TÃ¼rkÃ§e Ã–rnek - Duygu Analizi**

**Verilen:**
- 500 yorum test edildi
- 300 pozitif, 200 negatif
- SonuÃ§lar:
  - 250 pozitif doÄŸru tespit edildi
  - 50 pozitif yanlÄ±ÅŸ tespit edildi (negatif sanÄ±ldÄ±)
  - 30 negatif yanlÄ±ÅŸ tespit edildi (pozitif sanÄ±ldÄ±)
  - 170 negatif doÄŸru tespit edildi

**KarmaÅŸÄ±klÄ±k Matrisi:**
```
                Tahmin
GerÃ§ek    Pozitif    Negatif
Pozitif   250        50
Negatif   30         170
```

**Hesaplamalar:**
- TP = 250 (Pozitif doÄŸru tespit)
- FP = 30 (Negatif yanlÄ±ÅŸ pozitif)
- TN = 170 (Negatif doÄŸru tespit)
- FN = 50 (Pozitif yanlÄ±ÅŸ negatif)

---

### ğŸ“ **Ä°ngilizce Ã–rnek - Email Classification**

**Given:**
- 1200 emails tested
- 400 work, 800 personal
- Results:
  - 350 work emails correctly classified
  - 50 work emails misclassified as personal
  - 80 personal emails misclassified as work
  - 720 personal emails correctly classified

**Confusion Matrix:**
```
                Prediction
Actual      Work    Personal
Work        350     50
Personal    80      720
```

**Calculations:**
- TP = 350 (Work correctly classified)
- FP = 80 (Personal misclassified as work)
- TN = 720 (Personal correctly classified)
- FN = 50 (Work misclassified as personal)

### ğŸ“– **Ã–klid Mesafesi (Euclidean Distance)**

**FormÃ¼l:**
```
d(xâƒ—, yâƒ—) = âˆš(âˆ‘áµ¢â‚Œâ‚â¿ (xáµ¢ - yáµ¢)Â²)
```

**Ä°ngilizce Pratik Ã–rnek:**
```
DokÃ¼man A: [1, 2, 3, 0, 1] (the, cat, sat, on, mat)
DokÃ¼man B: [2, 1, 3, 1, 0] (the, dog, sat, on, floor)

d(A,B) = âˆš((1-2)Â² + (2-1)Â² + (3-3)Â² + (0-1)Â² + (1-0)Â²)
d(A,B) = âˆš(1 + 1 + 0 + 1 + 1) = âˆš4 = 2
```

**TÃ¼rkÃ§e Pratik Ã–rnek:**
```
DokÃ¼man A: [2, 1, 0, 1, 1] (kedi, evde, uyuyor, Ã§ok, mutlu)
DokÃ¼man B: [1, 2, 1, 0, 1] (kedi, bahÃ§ede, oynuyor, Ã§ok, mutlu)

d(A,B) = âˆš((2-1)Â² + (1-2)Â² + (0-1)Â² + (1-0)Â² + (1-1)Â²)
d(A,B) = âˆš(1 + 1 + 1 + 1 + 0) = âˆš4 = 2
```

### ğŸ“– **KosinÃ¼s BenzerliÄŸi (Cosine Similarity)**

**FormÃ¼l:**
```
cos(Î¸) = (xâƒ— Â· yâƒ—) / (||xâƒ—|| Ã— ||yâƒ—||)
```

**Ä°ngilizce Pratik Ã–rnek:**
```
DokÃ¼man A: [1, 2, 3, 0, 1] (the, cat, sat, on, mat)
DokÃ¼man B: [2, 1, 3, 1, 0] (the, dog, sat, on, floor)

A Â· B = 1Ã—2 + 2Ã—1 + 3Ã—3 + 0Ã—1 + 1Ã—0 = 2 + 2 + 9 + 0 + 0 = 13
||A|| = âˆš(1Â² + 2Â² + 3Â² + 0Â² + 1Â²) = âˆš15
||B|| = âˆš(2Â² + 1Â² + 3Â² + 1Â² + 0Â²) = âˆš15

cos(Î¸) = 13 / (âˆš15 Ã— âˆš15) = 13/15 â‰ˆ 0.867
```

**TÃ¼rkÃ§e Pratik Ã–rnek:**
```
DokÃ¼man A: [2, 1, 0, 1, 1] (kedi, evde, uyuyor, Ã§ok, mutlu)
DokÃ¼man B: [1, 2, 1, 0, 1] (kedi, bahÃ§ede, oynuyor, Ã§ok, mutlu)

A Â· B = 2Ã—1 + 1Ã—2 + 0Ã—1 + 1Ã—0 + 1Ã—1 = 2 + 2 + 0 + 0 + 1 = 5
||A|| = âˆš(2Â² + 1Â² + 0Â² + 1Â² + 1Â²) = âˆš7
||B|| = âˆš(1Â² + 2Â² + 1Â² + 0Â² + 1Â²) = âˆš7

cos(Î¸) = 5 / (âˆš7 Ã— âˆš7) = 5/7 â‰ˆ 0.714
```

### âš ï¸ **Farklar**
- **Ã–klid:** Mutlak mesafe, kÃ¼Ã§Ã¼k deÄŸer = benzer
- **KosinÃ¼s:** AÃ§Ä± benzerliÄŸi, bÃ¼yÃ¼k deÄŸer = benzer
- **KosinÃ¼s:** DokÃ¼man uzunluÄŸundan etkilenmez

### ğŸ“Š **KarÅŸÄ±laÅŸtÄ±rmalÄ± Analiz:**

| Ã–zellik | Ä°ngilizce | TÃ¼rkÃ§e |
|---------|-----------|--------|
| **Ã–klid Mesafesi** | 2.0 | 2.0 |
| **KosinÃ¼s BenzerliÄŸi** | 0.867 | 0.714 |
| **DokÃ¼man UzunluÄŸu** | 5 kelime | 5 kelime |
| **Ortak Kelimeler** | 3 (the, sat, on) | 3 (kedi, Ã§ok, mutlu) |
| **Benzerlik Seviyesi** | YÃ¼ksek | Orta | 

## ğŸ¯ MATRÄ°S YERLEÅÄ°MÄ° - ADIM ADIM REHBER

### ğŸ“Š 1. KARMAÅIKLIK MATRÄ°SÄ° (CONFUSION MATRIX) YERLEÅÄ°MÄ°

#### **AdÄ±m 1: Matris Boyutunu Belirle**
```
SÄ±nÄ±f sayÄ±sÄ± = n ise â†’ nÃ—n matris
Ä°kili sÄ±nÄ±flandÄ±rma iÃ§in: 2Ã—2 matris
```

#### **AdÄ±m 2: SatÄ±r ve SÃ¼tun BaÅŸlÄ±klarÄ±nÄ± YerleÅŸtir**
```
                Tahmin (Prediction)
GerÃ§ek (Actual)  SÄ±nÄ±f1    SÄ±nÄ±f2
SÄ±nÄ±f1           TP        FN
SÄ±nÄ±f2           FP        TN
```

#### **AdÄ±m 3: HÃ¼creleri Doldur**
```
TP (True Positive): DoÄŸru pozitif tahminler
FP (False Positive): YanlÄ±ÅŸ pozitif tahminler  
TN (True Negative): DoÄŸru negatif tahminler
FN (False Negative): YanlÄ±ÅŸ negatif tahminler
```

#### **AdÄ±m 4: Pratik Ã–rnek - AdÄ±m AdÄ±m**

**Verilen:**
- 1000 e-posta test edildi
- 200 spam, 800 normal
- SonuÃ§lar:
  - 150 spam doÄŸru tespit edildi
  - 50 spam yanlÄ±ÅŸ tespit edildi (normal sanÄ±ldÄ±)
  - 100 normal yanlÄ±ÅŸ tespit edildi (spam sanÄ±ldÄ±)
  - 700 normal doÄŸru tespit edildi

**AdÄ±m 1: Matris YapÄ±sÄ±nÄ± Ã‡iz**
```
                Tahmin
GerÃ§ek    Spam    Normal
Spam      [ ]     [ ]
Normal    [ ]     [ ]
```

**AdÄ±m 2: TP ve FN HÃ¼crelerini Doldur (Spam SatÄ±rÄ±)**
```
TP = 150 (Spam doÄŸru tespit)
FN = 50 (Spam yanlÄ±ÅŸ normal)

                Tahmin
GerÃ§ek    Spam    Normal
Spam      150     50
Normal    [ ]     [ ]
```

**AdÄ±m 3: FP ve TN HÃ¼crelerini Doldur (Normal SatÄ±rÄ±)**
```
FP = 100 (Normal yanlÄ±ÅŸ spam)
TN = 700 (Normal doÄŸru tespit)

                Tahmin
GerÃ§ek    Spam    Normal
Spam      150     50
Normal    100     700
```

**AdÄ±m 4: Kontrol Et**
```
Toplam Spam: 150 + 50 = 200 âœ“
Toplam Normal: 100 + 700 = 800 âœ“
Toplam Test: 200 + 800 = 1000 âœ“
```

---

### ğŸ“Š 2. CYK TABLOSU YERLEÅÄ°MÄ°

#### **AdÄ±m 1: CÃ¼mle UzunluÄŸunu Belirle**
```
CÃ¼mle: "the cat chased the dog"
Uzunluk: 5 kelime â†’ 5Ã—5 tablo
```

#### **AdÄ±m 2: Tablo YapÄ±sÄ±nÄ± OluÅŸtur**
```
     1     2     3     4     5
1   [ ]   [ ]   [ ]   [ ]   [ ]
2   [ ]   [ ]   [ ]   [ ]   [ ]
3   [ ]   [ ]   [ ]   [ ]   [ ]
4   [ ]   [ ]   [ ]   [ ]   [ ]
5   [ ]   [ ]   [ ]   [ ]   [ ]
```

#### **AdÄ±m 3: KÃ¶ÅŸegeni Doldur (Tek Kelimeler)**
```
[1,1]: "the" â†’ Det
[2,2]: "cat" â†’ N
[3,3]: "chased" â†’ V
[4,4]: "the" â†’ Det
[5,5]: "dog" â†’ N

     1     2     3     4     5
1   Det   [ ]   [ ]   [ ]   [ ]
2   [ ]   N     [ ]   [ ]   [ ]
3   [ ]   [ ]   V     [ ]   [ ]
4   [ ]   [ ]   [ ]   Det   [ ]
5   [ ]   [ ]   [ ]   [ ]   N
```

#### **AdÄ±m 4: 2'li GruplarÄ± Doldur (Uzunluk 2)**
```
[1,2]: Det + N â†’ NP
[2,3]: N + V â†’ (geÃ§ersiz)
[3,4]: V + Det â†’ (geÃ§ersiz)
[4,5]: Det + N â†’ NP

     1     2     3     4     5
1   Det   NP    [ ]   [ ]   [ ]
2   [ ]   N     [ ]   [ ]   [ ]
3   [ ]   [ ]   V     [ ]   [ ]
4   [ ]   [ ]   [ ]   Det   NP
5   [ ]   [ ]   [ ]   [ ]   N
```

#### **AdÄ±m 5: 3'lÃ¼ GruplarÄ± Doldur (Uzunluk 3)**
```
[1,3]: Det + N + V â†’ (geÃ§ersiz)
[2,4]: N + V + Det â†’ (geÃ§ersiz)
[3,5]: V + Det + N â†’ VP

     1     2     3     4     5
1   Det   NP    [ ]   [ ]   [ ]
2   [ ]   N     [ ]   [ ]   [ ]
3   [ ]   [ ]   V     [ ]   [ ]
4   [ ]   [ ]   [ ]   Det   NP
5   [ ]   [ ]   [ ]   [ ]   N
```

#### **AdÄ±m 6: 4'lÃ¼ GruplarÄ± Doldur (Uzunluk 4)**
```
[1,4]: Det + N + V + Det â†’ (geÃ§ersiz)
[2,5]: N + V + Det + N â†’ (geÃ§ersiz)
```

#### **AdÄ±m 7: 5'li Grubu Doldur (Uzunluk 5)**
```
[1,5]: Det + N + V + Det + N â†’ S

     1     2     3     4     5
1   Det   NP    [ ]   [ ]   S
2   [ ]   N     [ ]   [ ]   [ ]
3   [ ]   [ ]   V     [ ]   [ ]
4   [ ]   [ ]   [ ]   Det   NP
5   [ ]   [ ]   [ ]   [ ]   N
```

---

### ğŸ“Š 3. TÃœRKÃ‡E CYK TABLOSU Ã–RNEÄÄ°

#### **CÃ¼mle:** "bu kedi kÃ¶peÄŸi kovaladÄ±"

#### **AdÄ±m 1: 4Ã—4 Tablo OluÅŸtur**
```
     1     2     3     4
1   [ ]   [ ]   [ ]   [ ]
2   [ ]   [ ]   [ ]   [ ]
3   [ ]   [ ]   [ ]   [ ]
4   [ ]   [ ]   [ ]   [ ]
```

#### **AdÄ±m 2: KÃ¶ÅŸegeni Doldur**
```
[1,1]: "bu" â†’ Det
[2,2]: "kedi" â†’ N
[3,3]: "kÃ¶peÄŸi" â†’ N
[4,4]: "kovaladÄ±" â†’ V

     1     2     3     4
1   Det   [ ]   [ ]   [ ]
2   [ ]   N     [ ]   [ ]
3   [ ]   [ ]   N     [ ]
4   [ ]   [ ]   [ ]   V
```

#### **AdÄ±m 3: 2'li GruplarÄ± Doldur**
```
[1,2]: Det + N â†’ NP
[2,3]: N + N â†’ (geÃ§ersiz)
[3,4]: N + V â†’ VP

     1     2     3     4
1   Det   NP    [ ]   [ ]
2   [ ]   N     [ ]   [ ]
3   [ ]   [ ]   N     [ ]
4   [ ]   [ ]   [ ]   V
```

#### **AdÄ±m 4: 3'lÃ¼ GruplarÄ± Doldur**
```
[1,3]: Det + N + N â†’ (geÃ§ersiz)
[2,4]: N + N + V â†’ (geÃ§ersiz)
```

#### **AdÄ±m 5: 4'lÃ¼ Grubu Doldur**
```
[1,4]: Det + N + N + V â†’ S

     1     2     3     4
1   Det   NP    [ ]   S
2   [ ]   N     [ ]   [ ]
3   [ ]   [ ]   N     [ ]
4   [ ]   [ ]   [ ]   V
```

---

### ğŸ“Š 4. MATRÄ°S YERLEÅÄ°M KURALLARI

#### **KarmaÅŸÄ±klÄ±k Matrisi KurallarÄ±:**
1. **SatÄ±rlar:** GerÃ§ek deÄŸerler
2. **SÃ¼tunlar:** Tahmin edilen deÄŸerler
3. **KÃ¶ÅŸegen:** DoÄŸru tahminler (TP, TN)
4. **KÃ¶ÅŸegen dÄ±ÅŸÄ±:** YanlÄ±ÅŸ tahminler (FP, FN)
5. **SatÄ±r toplamlarÄ±:** GerÃ§ek sÄ±nÄ±f sayÄ±larÄ±
6. **SÃ¼tun toplamlarÄ±:** Tahmin edilen sÄ±nÄ±f sayÄ±larÄ±

#### **CYK Tablosu KurallarÄ±:**
1. **Boyut:** nÃ—n (n = cÃ¼mle uzunluÄŸu)
2. **KÃ¶ÅŸegen:** Tek kelimeler (uzunluk 1)
3. **Ãœst kÃ¶ÅŸegenler:** Ã‡oklu kelime gruplarÄ±
4. **[i,j] hÃ¼cresi:** i'den j'ye kadar olan kelimeler
5. **Doldurma sÄ±rasÄ±:** KÃ¶ÅŸegenden baÅŸla, yukarÄ± Ã§Ä±k
6. **Son kontrol:** [1,n] hÃ¼cresinde S var mÄ±?

#### **Genel Matris KurallarÄ±:**
1. **SatÄ±r numaralarÄ±:** Soldan saÄŸa
2. **SÃ¼tun numaralarÄ±:** YukarÄ±dan aÅŸaÄŸÄ±
3. **HÃ¼cre koordinatlarÄ±:** [satÄ±r, sÃ¼tun]
4. **BoÅŸ hÃ¼creler:** "-" veya boÅŸ bÄ±rak
5. **Kontrol:** Toplamlar tutarlÄ± olmalÄ±

---

### ğŸ“Š 5. SINAV Ä°PUÃ‡LARI

#### **KarmaÅŸÄ±klÄ±k Matrisi:**
- **Ã–nce yapÄ±yÄ± Ã§iz** (satÄ±r/sÃ¼tun baÅŸlÄ±klarÄ±)
- **TP ve TN'yi kÃ¶ÅŸegene** yerleÅŸtir
- **FP ve FN'yi kÃ¶ÅŸegen dÄ±ÅŸÄ±na** yerleÅŸtir
- **ToplamlarÄ± kontrol et**

#### **CYK Tablosu:**
- **Ã–nce boyutu belirle** (cÃ¼mle uzunluÄŸu)
- **KÃ¶ÅŸegeni doldur** (tek kelimeler)
- **YukarÄ± Ã§Ä±k** (2'li, 3'lÃ¼, 4'lÃ¼ gruplar)
- **Gramer kurallarÄ±nÄ±** uygula
- **Son hÃ¼creyi kontrol et** ([1,n])

#### **Hata KontrolÃ¼:**
- **Matris boyutu** doÄŸru mu?
- **Toplamlar** tutarlÄ± mÄ±?
- **Gramer kurallarÄ±** uygulandÄ± mÄ±?
- **SonuÃ§** mantÄ±klÄ± mÄ±?

---

**ğŸ“ BAÅARILAR! Matris yerleÅŸiminde dikkatli olun! ğŸ“** 

## ğŸ¯ SLAYT KONULARI - DETAYLI ANALÄ°Z

### ğŸ“Š 1. ZIPF YASASI - DETAYLI AÃ‡IKLAMA

#### **ğŸ“– Teorik Temel:**
Zipf yasasÄ±, **George Kingsley Zipf** tarafÄ±ndan 1935'te keÅŸfedilen, dildeki kelime frekanslarÄ± ile sÄ±ralamalarÄ± arasÄ±ndaki matematiksel iliÅŸkiyi aÃ§Ä±klayan bir yasadÄ±r.

#### **ğŸ”¢ Matematiksel FormÃ¼l:**
```
Zipf = R Ã— F / N_toplam
```
- **R:** Kelimenin sÄ±ralamasÄ±
- **F:** Kelimenin frekansÄ±
- **N_toplam:** Toplam kelime sayÄ±sÄ±

#### **ğŸ“ Slayttaki Ã–rnek:**
```
N_toplam = 90800
"ile" kelimesi: R = 2, F = 676
Zipf = 2 Ã— 676 / 90800 = 0.022

"ile" kelimesi: R = 6, F = 511  
Zipf = 6 Ã— 511 / 90800 = 0.033
```

#### **ğŸ¯ Pratik Uygulama:**
**TÃ¼rkÃ§e Ã–rnek:**
```
Toplam kelime sayÄ±sÄ±: 50000
"bir" kelimesi: 1. sÄ±rada, 2500 kez geÃ§iyor
"ve" kelimesi: 2. sÄ±rada, 1800 kez geÃ§iyor

Zipf("bir") = 1 Ã— 2500 / 50000 = 0.05
Zipf("ve") = 2 Ã— 1800 / 50000 = 0.072
```

#### **âš ï¸ Ã–nemli Noktalar:**
- **Enerji korunumu:** En sÄ±k kullanÄ±lan kelimeler az sayÄ±da
- **DoÄŸal sÄ±ralama:** Yeni kelimeler eklendiÄŸinde sÄ±ralama deÄŸiÅŸir
- **Sabit oran:** Frekans ile sÄ±ralama arasÄ±nda sabit iliÅŸki

---

### ğŸ“Š 2. CÃœMLE OLASILIKLARI - DETAYLI AÃ‡IKLAMA

#### **ğŸ“– Teorik Temel:**
Bir cÃ¼mlenin olasÄ±lÄ±ÄŸÄ±, cÃ¼mleyi oluÅŸturan kelimelerin olasÄ±lÄ±klarÄ±nÄ±n Ã§arpÄ±mÄ±dÄ±r.

#### **ğŸ”¢ Matematiksel FormÃ¼l:**
```
P(wâ‚, wâ‚‚, ..., wâ‚™) = P(wâ‚) Ã— P(wâ‚‚) Ã— ... Ã— P(wâ‚™)
```

#### **ğŸ“ Slayttaki Ã–rnek:**
```
P(w = bir) = frekans(bir) / toplam = 3180/10900 = 0.292
```

#### **ğŸ¯ Pratik Uygulama:**

**TÃ¼rkÃ§e Ã–rnek:**
```
"bugÃ¼n hava gÃ¼zel" cÃ¼mlesi iÃ§in:
P(bugÃ¼n) = 150/5000 = 0.03
P(hava) = 200/5000 = 0.04  
P(gÃ¼zel) = 100/5000 = 0.02

P(cÃ¼mle) = 0.03 Ã— 0.04 Ã— 0.02 = 0.000024
```

**Ä°ngilizce Ã–rnek:**
```
"the cat sat" cÃ¼mlesi iÃ§in:
P(the) = 500/3000 = 0.167
P(cat) = 50/3000 = 0.017
P(sat) = 30/3000 = 0.01

P(cÃ¼mle) = 0.167 Ã— 0.017 Ã— 0.01 = 0.000028
```

#### **âš ï¸ Ã–nemli Noktalar:**
- **BaÄŸÄ±msÄ±zlÄ±k varsayÄ±mÄ±:** Kelimeler birbirinden baÄŸÄ±msÄ±z
- **SÄ±ra Ã¶nemsiz:** "yÃ¼z oldu" = "oldu yÃ¼z"
- **Ã‡ok kÃ¼Ã§Ã¼k deÄŸerler:** Uzun cÃ¼mleler iÃ§in Ã§ok kÃ¼Ã§Ã¼k olasÄ±lÄ±klar

---

### ğŸ“Š 3. ENTROPÄ° - DETAYLI AÃ‡IKLAMA

#### **ğŸ“– Teorik Temel:**
Entropi, bir sistemdeki belirsizliÄŸi veya rastgeleliÄŸi Ã¶lÃ§en bir kavramdÄ±r. Bilgi teorisinde, bir mesajÄ±n taÅŸÄ±dÄ±ÄŸÄ± bilgi miktarÄ±nÄ± Ã¶lÃ§mek iÃ§in kullanÄ±lÄ±r.

#### **ğŸ”¢ Matematiksel FormÃ¼l:**
```
H(x) = âˆ‘ P(x) Ã— log(1/P(x))
```

#### **ğŸ“ Slayttaki Ã–rnek:**
```
AVM'ye gelen arabalar:
P(sedan) = 100/200 = 0.5
P(hatchback) = 50/200 = 0.25
P(station) = 25/200 = 0.125
P(sport) = 25/200 = 0.125

H(x) = 0.5Ã—1 + 0.25Ã—2 + 2Ã—0.125Ã—3 = 1.5
```

#### **ğŸ¯ Pratik Uygulama:**

**TÃ¼rkÃ§e Ã–rnek - Hava Durumu:**
```
P(gÃ¼neÅŸli) = 0.4
P(yaÄŸmurlu) = 0.3
P(bulutlu) = 0.2
P(karlÄ±) = 0.1

H(x) = 0.4Ã—1.32 + 0.3Ã—1.74 + 0.2Ã—2.32 + 0.1Ã—3.32 = 1.85
```

**Ä°ngilizce Ã–rnek - Email TÃ¼rleri:**
```
P(work) = 0.5
P(personal) = 0.3
P(spam) = 0.2

H(x) = 0.5Ã—1 + 0.3Ã—1.74 + 0.2Ã—2.32 = 1.49
```

#### **âš ï¸ Ã–nemli Noktalar:**
- **Maksimum entropi:** EÅŸit olasÄ±lÄ±klar iÃ§in
- **Minimum entropi:** Tek olasÄ±lÄ±k iÃ§in (0)
- **Bilgi Ã¶lÃ§Ã¼mÃ¼:** YÃ¼ksek entropi = Ã§ok bilgi

---

### ğŸ“Š 4. PERPLEXITY - DETAYLI AÃ‡IKLAMA

#### **ğŸ“– Teorik Temel:**
Perplexity, bir dil modelinin ne kadar iyi Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± Ã¶lÃ§en bir metriktir. DÃ¼ÅŸÃ¼k perplexity deÄŸeri, modelin daha iyi olduÄŸunu gÃ¶sterir.

#### **ğŸ”¢ Matematiksel FormÃ¼l:**
```
Perplexity = 2^H(x)
```
H(x) = Cross-entropy

#### **ğŸ“ Slayttaki AÃ§Ä±klama:**
- **BÃ¼yÃ¼k metin kÃ¼mesi** yerine **model olasÄ±lÄ±klarÄ±** kullanÄ±lÄ±r
- **GerÃ§ek dÃ¼nyaya yakÄ±nsama** saÄŸlanÄ±r
- **Model performansÄ±** Ã¶lÃ§Ã¼lÃ¼r

#### **ğŸ¯ Pratik Uygulama:**

**TÃ¼rkÃ§e Ã–rnek:**
```
Model tahminleri:
P(bugÃ¼n|hava) = 0.3
P(hava|gÃ¼zel) = 0.4
P(gÃ¼zel|.) = 0.2

Cross-entropy = -(log(0.3) + log(0.4) + log(0.2)) / 3 = 1.74
Perplexity = 2^1.74 = 3.35
```

**Ä°ngilizce Ã–rnek:**
```
Model tahminleri:
P(the|cat) = 0.1
P(cat|sat) = 0.05
P(sat|.) = 0.2

Cross-entropy = -(log(0.1) + log(0.05) + log(0.2)) / 3 = 3.32
Perplexity = 2^3.32 = 10.0
```

---

### ğŸ“Š 5. DÄ°L MODELLERÄ° - DETAYLI AÃ‡IKLAMA

#### **ğŸ“– Teorik Temel:**
Dil modelleri, kelime dizilerinin olasÄ±lÄ±klarÄ±nÄ± hesaplayan modellerdir. Markov Ã¶zelliÄŸi kullanarak, bir kelimenin olasÄ±lÄ±ÄŸÄ± sadece Ã¶nceki kelimelere baÄŸlÄ±dÄ±r.

#### **ğŸ”¢ Matematiksel FormÃ¼l:**
```
P(wâ‚, wâ‚‚, ..., wâ‚™) = P(wâ‚|BAÅLANGIÃ‡) Ã— P(wâ‚‚|wâ‚) Ã— P(wâ‚ƒ|wâ‚‚) Ã— ... Ã— P(wâ‚™|wâ‚™â‚‹â‚)
```

#### **ğŸ“ Slayttaki Ã–rnek:**
```
"SavaÅŸ tazminatÄ± aldÄ±lar." cÃ¼mlesi iÃ§in:
P(cÃ¼mle) = P(savaÅŸ|BAÅLANGIÃ‡) Ã— P(tazminatÄ±|savaÅŸ) Ã— P(aldÄ±lar|tazminatÄ±) Ã— P(.|aldÄ±lar)
```

#### **ğŸ¯ Pratik Uygulama:**

**TÃ¼rkÃ§e Ã–rnek:**
```
"BugÃ¼n hava Ã§ok gÃ¼zel" cÃ¼mlesi iÃ§in:
P(bugÃ¼n|BAÅLANGIÃ‡) = 0.1
P(hava|bugÃ¼n) = 0.3
P(Ã§ok|hava) = 0.2
P(gÃ¼zel|Ã§ok) = 0.4
P(.|gÃ¼zel) = 0.8

P(cÃ¼mle) = 0.1 Ã— 0.3 Ã— 0.2 Ã— 0.4 Ã— 0.8 = 0.00192
```

**Ä°ngilizce Ã–rnek:**
```
"The cat sat on mat" cÃ¼mlesi iÃ§in:
P(the|BAÅLANGIÃ‡) = 0.2
P(cat|the) = 0.1
P(sat|cat) = 0.05
P(on|sat) = 0.3
P(mat|on) = 0.2
P(.|mat) = 0.9

P(cÃ¼mle) = 0.2 Ã— 0.1 Ã— 0.05 Ã— 0.3 Ã— 0.2 Ã— 0.9 = 0.000054
```

#### **âš ï¸ Ã–nemli Noktalar:**
- **Markov Ã¶zelliÄŸi:** Sadece Ã¶nceki kelimeye baÄŸlÄ±
- **KoÅŸullu olasÄ±lÄ±k:** P(wáµ¢|wáµ¢â‚‹â‚) hesaplama
- **Smoothing:** SÄ±fÄ±r olasÄ±lÄ±k problemini Ã§Ã¶zme

---

### ğŸ“Š 6. EÅDÄ°ZÄ°MLÄ°LÄ°K (COLLOCATION) - DETAYLI AÃ‡IKLAMA

#### **ğŸ“– Teorik Temel:**
EÅŸdizimlilik, kelimelerin birlikte sÄ±k geÃ§mesi durumudur. Bu kelimeler anlamlÄ± bir bÃ¼tÃ¼n oluÅŸturur.

#### **ğŸ”¢ Pointwise Mutual Information (PMI):**
```
I(x,y) = logâ‚‚(P(x,y) / (P(x) Ã— P(y)))
```

#### **ğŸ“ Slayttaki Ã–rnek:**
```
"New York" kelimesi iÃ§in:
P(New) = 541/1500 = 0.36
P(York) = 212/1500 = 0.14
P(New York) = 5/1500 = 0.003

PMI = logâ‚‚(0.003 / (0.36 Ã— 0.14)) = logâ‚‚(0.003 / 0.0504) = logâ‚‚(0.0595) = -4.07
```

#### **ğŸ¯ Pratik Uygulama:**

**TÃ¼rkÃ§e Ã–rnek:**
```
"zaman kaybÄ±" kelimesi iÃ§in:
P(zaman) = 200/1000 = 0.2
P(kaybÄ±) = 50/1000 = 0.05
P(zaman kaybÄ±) = 15/1000 = 0.015

PMI = logâ‚‚(0.015 / (0.2 Ã— 0.05)) = logâ‚‚(0.015 / 0.01) = logâ‚‚(1.5) = 0.585
```

**Ä°ngilizce Ã–rnek:**
```
"fast food" kelimesi iÃ§in:
P(fast) = 150/2000 = 0.075
P(food) = 300/2000 = 0.15
P(fast food) = 25/2000 = 0.0125

PMI = logâ‚‚(0.0125 / (0.075 Ã— 0.15)) = logâ‚‚(0.0125 / 0.01125) = logâ‚‚(1.11) = 0.15
```

#### **âš ï¸ Null Hipotez Testi:**
```
Hâ‚€: P(x) Ã— P(y) > P(x,y) â†’ BaÄŸÄ±msÄ±z (eÅŸdizim deÄŸil)
Hâ‚: P(x,y) > P(x) Ã— P(y) â†’ EÅŸdizim
```

---

### ğŸ“Š 7. INTERPOLASYON - DETAYLI AÃ‡IKLAMA

#### **ğŸ“– Teorik Temel:**
Interpolasyon, seyrek geÃ§en kelimeler iÃ§in smoothing tekniÄŸidir. FarklÄ± n-gram modellerinin aÄŸÄ±rlÄ±klÄ± ortalamasÄ±nÄ± alÄ±r.

#### **ğŸ”¢ Matematiksel FormÃ¼l:**
```
P(wâ‚™|wâ‚™â‚‹â‚‚,wâ‚™â‚‹â‚) = Î»â‚P(wâ‚™) + Î»â‚‚P(wâ‚™|wâ‚™â‚‹â‚) + Î»â‚ƒP(wâ‚™|wâ‚™â‚‹â‚‚,wâ‚™â‚‹â‚)
```

#### **ğŸ“ Slayttaki AÃ§Ä±klama:**
- **SÄ±fÄ±r olasÄ±lÄ±k problemi** Ã§Ã¶zÃ¼lÃ¼r
- **Lambda katsayÄ±larÄ±** pozitif deÄŸerler
- **FarklÄ± n-gram seviyeleri** birleÅŸtirilir

#### **ğŸ¯ Pratik Uygulama:**

**TÃ¼rkÃ§e Ã–rnek:**
```
"zamazingo" kelimesi iÃ§in:
P(zamazingo) = 0 (hiÃ§ geÃ§memiÅŸ)
P(zamazingo|Ã¶nceki) = 0.001
P(zamazingo|Ã¶nceki,Ã¶nceki) = 0.0001

Î»â‚ = 0.1, Î»â‚‚ = 0.3, Î»â‚ƒ = 0.6

P(zamazingo|Ã¶nceki,Ã¶nceki) = 0.1Ã—0 + 0.3Ã—0.001 + 0.6Ã—0.0001 = 0.00036
```

**Ä°ngilizce Ã–rnek:**
```
"supercalifragilistic" kelimesi iÃ§in:
P(supercalifragilistic) = 0
P(supercalifragilistic|previous) = 0.0005
P(supercalifragilistic|prev,prev) = 0.0001

Î»â‚ = 0.2, Î»â‚‚ = 0.4, Î»â‚ƒ = 0.4

P(supercalifragilistic|prev,prev) = 0.2Ã—0 + 0.4Ã—0.0005 + 0.4Ã—0.0001 = 0.00024
```

---

### ğŸ“Š 8. KELÄ°ME TORBASI (BAG OF WORDS) - DETAYLI AÃ‡IKLAMA

#### **ğŸ“– Teorik Temel:**
Kelime torbasÄ±, bir dokÃ¼manÄ± kelime frekanslarÄ±na gÃ¶re vektÃ¶r olarak temsil eden yaklaÅŸÄ±mdÄ±r. Kelimelerin sÄ±rasÄ± Ã¶nemsizdir.

#### **ğŸ”¢ Ä°ÅŸlem AdÄ±mlarÄ±:**
1. **Kelime sÄ±nÄ±rlarÄ±nÄ±n bulunmasÄ±**
2. **Eklerin kaldÄ±rÄ±lmasÄ± (stemming)**
3. **SÃ¶zlÃ¼k oluÅŸturma**
4. **DokÃ¼man vektÃ¶rÃ¼ oluÅŸturma**

#### **ğŸ“ Slayttaki Ã–rnek:**
```
DokÃ¼man: "Edirne'de MeriÃ§, Tunca ve Arda nehirleri etrafÄ±nda 2315 parÃ§a bakÄ±mlÄ± sebze, meyve ve dut bahÃ§eleri vardÄ±r."

Kelime Ã‡Ä±karÄ±mÄ±: [edirne, meriÃ§, tunca, arda, nehirleri, etrafÄ±nda, 2315, parÃ§a, bakÄ±mlÄ±, sebze, meyve, ve, dut, bahÃ§eleri, vardÄ±r]

SÃ¶zlÃ¼k: {edirne, meriÃ§, tunca, arda, nehir, etraf, parÃ§a, bakÄ±m, sebze, meyve, dut, yunanistan, savaÅŸ tazminatÄ±, lozan antlaÅŸmasÄ±, karaaÄŸaÃ§, tÃ¼rkiye, lozan anÄ±tÄ±, ilÃ§e, inÅŸa, ve, dan, ile, diÄŸer, ol, edilen, aldÄ±, mahale, var, bol, al, katÄ±l, 15 EylÃ¼l 1923, 2315}
```

#### **ğŸ¯ Pratik Uygulama:**

**TÃ¼rkÃ§e Ã–rnek:**
```
DokÃ¼man: "Ä°stanbul'da hava bugÃ¼n Ã§ok gÃ¼zel. BoÄŸaz manzarasÄ± muhteÅŸem."

Kelime Ã‡Ä±karÄ±mÄ±: [istanbul, da, hava, bugÃ¼n, Ã§ok, gÃ¼zel, boÄŸaz, manzara, muhteÅŸem]

SÃ¶zlÃ¼k: {istanbul, hava, bugÃ¼n, Ã§ok, gÃ¼zel, boÄŸaz, manzara, muhteÅŸem, ...}

DokÃ¼man VektÃ¶rÃ¼: [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...]
```

**Ä°ngilizce Ã–rnek:**
```
Document: "The weather in London is beautiful today. The Thames looks amazing."

Word Extraction: [the, weather, in, london, is, beautiful, today, thames, looks, amazing]

Dictionary: {the, weather, in, london, is, beautiful, today, thames, looks, amazing, ...}

Document Vector: [2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...]
```

#### **âš ï¸ Ã–nemli Noktalar:**
- **SÄ±ra Ã¶nemsiz:** Kelimelerin dizilimi Ã¶nemli deÄŸil
- **Frekans Ã¶nemli:** AynÄ± kelime birden fazla geÃ§ebilir
- **Boyut problemi:** BÃ¼yÃ¼k sÃ¶zlÃ¼kler iÃ§in seyrek vektÃ¶rler

---

### ğŸ“Š 9. DOKÃœMAN VEKTÃ–RÃœ - DETAYLI AÃ‡IKLAMA

#### **ğŸ“– Teorik Temel:**
DokÃ¼man vektÃ¶rÃ¼, bir dokÃ¼manÄ± sayÄ±sal olarak temsil eden vektÃ¶rdÃ¼r. Her boyut bir kelimeyi temsil eder.

#### **ğŸ”¢ VektÃ¶r TÃ¼rleri:**
1. **GeÃ§ip/geÃ§meme:** 1 veya 0
2. **GeÃ§me sayÄ±sÄ±:** Rakamla
3. **AÄŸÄ±rlÄ±k:** TF-IDF ile

#### **ğŸ“ Slayttaki Ã–rnek:**
```
DokÃ¼man: "Edirne'de MeriÃ§, Tunca ve Arda nehirleri etrafÄ±nda 2315 parÃ§a bakÄ±mlÄ± sebze, meyve ve dut bahÃ§eleri vardÄ±r."

GeÃ§ip/geÃ§meme durumu: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, ..., 1, 0, 0, 0, 1]

GeÃ§me sayÄ±sÄ±: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 0, 0, 0, 0, 2, ..., 1, 0, 0, 0, 1]
```

#### **ğŸ¯ Pratik Uygulama:**

**TÃ¼rkÃ§e Ã–rnek:**
```
DokÃ¼man: "Kedi evde uyuyor. Kedi Ã§ok mutlu."

SÃ¶zlÃ¼k: [kedi, evde, uyuyor, Ã§ok, mutlu, kÃ¶pek, bahÃ§ede, oynuyor]

VektÃ¶r (geÃ§ip/geÃ§meme): [1, 1, 1, 1, 1, 0, 0, 0]
VektÃ¶r (geÃ§me sayÄ±sÄ±): [2, 1, 1, 1, 1, 0, 0, 0]
```

**Ä°ngilizce Ã–rnek:**
```
Document: "The cat sat on the mat. The cat is happy."

Dictionary: [the, cat, sat, on, mat, is, happy, dog, ran]

Vector (binary): [1, 1, 1, 1, 1, 1, 1, 0, 0]
Vector (count): [2, 2, 1, 1, 1, 1, 1, 0, 0]
```

#### **âš ï¸ DokÃ¼man UzunluÄŸu:**
```
Norm hesaplama: ||x|| = âˆš(xâ‚Â² + xâ‚‚Â² + ... + xâ‚™Â²)

Ã–rnek: [1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 0, 0, 2, 2, 1, 1, ..., 1, 0, 3, 5, 1]
Norm = âˆš(1Â² + 1Â² + 1Â² + 1Â² + 2Â² + 1Â² + ... + 1Â²) = âˆš(14 + 58 + 9) = 9
```

---

### ğŸ“Š 10. SINIFLANDIRMA TÃœRLERÄ° - DETAYLI AÃ‡IKLAMA

#### **ğŸ“– Teorik Temel:**
Metin sÄ±nÄ±flandÄ±rma, dokÃ¼manlarÄ± belirli kategorilere atama iÅŸlemidir.

#### **ğŸ”¢ SÄ±nÄ±flandÄ±rma TÃ¼rleri:**

**1. Ä°kili SÄ±nÄ±flandÄ±rma (Binary Classification):**
```
Ã–rnek: Spam/Normal e-posta
- 2 sÄ±nÄ±ftan sadece biri
- P(sÄ±nÄ±f1) + P(sÄ±nÄ±f2) = 1
```

**2. Ã‡ok SÄ±nÄ±flÄ± SÄ±nÄ±flandÄ±rma (Multiclass Classification):**
```
Ã–rnek: Spor, Politika, Ekonomi, YaÅŸam
- Birden fazla sÄ±nÄ±ftan sadece biri
- P(sÄ±nÄ±f1) + P(sÄ±nÄ±f2) + ... + P(sÄ±nÄ±fN) = 1
```

**3. Ã‡ok Etiketli SÄ±nÄ±flandÄ±rma (Multilabel Classification):**
```
Ã–rnek: Bir dokÃ¼man hem Spor hem Ekonomi olabilir
- Birden fazla sÄ±nÄ±fa aynÄ± anda ait olabilir
- Her sÄ±nÄ±f iÃ§in ayrÄ± olasÄ±lÄ±k
```

#### **ğŸ¯ Pratik Uygulama:**

**TÃ¼rkÃ§e Ã–rnekler:**
```
Ä°kili: Spam/Normal SMS
Ã‡ok SÄ±nÄ±flÄ±: Haber kategorileri (Spor, Politika, Ekonomi)
Ã‡ok Etiketli: Duygu analizi (Mutlu, ÃœzgÃ¼n, KÄ±zgÄ±n aynÄ± anda)
```

**Ä°ngilizce Ã–rnekler:**
```
Binary: Work/Personal email
Multiclass: News categories (Sports, Politics, Economy)
Multilabel: Sentiment analysis (Happy, Sad, Angry simultaneously)
```

---

### ğŸ“Š 11. K-EN YAKIN KOMÅU (K-NN) - DETAYLI AÃ‡IKLAMA

#### **ğŸ“– Teorik Temel:**
K-NN, bir dokÃ¼manÄ± en yakÄ±n k komÅŸusuna bakarak sÄ±nÄ±flandÄ±ran algoritmadÄ±r.

#### **ğŸ”¢ Algoritma AdÄ±mlarÄ±:**
1. **Mesafe hesaplama** (Ã–klid, KosinÃ¼s)
2. **En yakÄ±n k komÅŸu bulma**
3. **Ã‡oÄŸunluk oylamasÄ±**

#### **ğŸ“ Slayttaki AÃ§Ä±klama:**
- **K deÄŸeri:** En yakÄ±n k adet komÅŸu
- **YoÄŸunluk hesaplama:** KomÅŸular arasÄ±nda en Ã§ok hangi sÄ±nÄ±f
- **Merkez yaklaÅŸÄ±mÄ±:** SÄ±nÄ±f merkez vektÃ¶rÃ¼ne en yakÄ±n

#### **ğŸ¯ Pratik Uygulama:**

**TÃ¼rkÃ§e Ã–rnek:**
```
Yeni dokÃ¼man: [1, 0, 1, 1, 0, 1, 0, 0]
K = 3

En yakÄ±n 3 komÅŸu:
- KomÅŸu 1: Spor sÄ±nÄ±fÄ±, mesafe = 1.5
- KomÅŸu 2: Spor sÄ±nÄ±fÄ±, mesafe = 2.1  
- KomÅŸu 3: Politika sÄ±nÄ±fÄ±, mesafe = 2.3

SonuÃ§: Spor sÄ±nÄ±fÄ± (2/3 oy)
```

**Ä°ngilizce Ã–rnek:**
```
New document: [1, 1, 0, 1, 0, 1, 0, 0]
K = 5

Nearest 5 neighbors:
- Neighbor 1: Sports class, distance = 1.2
- Neighbor 2: Sports class, distance = 1.8
- Neighbor 3: Politics class, distance = 2.0
- Neighbor 4: Sports class, distance = 2.1
- Neighbor 5: Economy class, distance = 2.5

Result: Sports class (3/5 votes)
```

#### **âš ï¸ Merkez YaklaÅŸÄ±mÄ±:**
```
Her sÄ±nÄ±f iÃ§in ortalama vektÃ¶r hesapla
Yeni dokÃ¼manÄ± en yakÄ±n merkeze ata

Spor merkezi: [0.8, 0.2, 0.9, 0.1, 0.3, 0.7, 0.1, 0.2]
Politika merkezi: [0.1, 0.9, 0.2, 0.8, 0.7, 0.1, 0.8, 0.1]
```

---

### ğŸ“Š 12. DEÄERLENDÄ°RME Ã–LÃ‡ÃœTLERÄ° - DETAYLI AÃ‡IKLAMA

#### **ğŸ“– Teorik Temel:**
SÄ±nÄ±flandÄ±rma performansÄ±nÄ± Ã¶lÃ§mek iÃ§in kullanÄ±lan metriklerdir.

#### **ğŸ”¢ Temel Metrikler:**

**1. DoÄŸruluk (Accuracy):**
```
Accuracy = (TP + TN) / (TP + TN + FP + FN)
```

**2. Kesinlik (Precision):**
```
Precision = TP / (TP + FP)
```

**3. DuyarlÄ±lÄ±k (Recall):**
```
Recall = TP / (TP + FN)
```

**4. F-Ã–lÃ§Ã¼sÃ¼ (F-Measure):**
```
F-Measure = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
```

#### **ğŸ“ Slayttaki Ã–rnek:**
```
1000 SMS: 200 Normal, 800 Spam
Normal: 100 doÄŸru, 100 yanlÄ±ÅŸ
Spam: 300 doÄŸru, 500 yanlÄ±ÅŸ

Ham ortalama = (100/200 + 300/800) / 2 = 0.4375 = 43.75%
AÄŸÄ±rlÄ±klÄ± ortalama = (200/1000 Ã— 100/200) + (800/1000 Ã— 300/800) = 0.4 = 40%
```

#### **ğŸ¯ Pratik Uygulama:**

**TÃ¼rkÃ§e Ã–rnek:**
```
500 yorum: 300 pozitif, 200 negatif
Pozitif: 250 doÄŸru, 50 yanlÄ±ÅŸ
Negatif: 170 doÄŸru, 30 yanlÄ±ÅŸ

TP = 250, FP = 30, TN = 170, FN = 50

Accuracy = (250 + 170) / 500 = 84%
Precision = 250 / (250 + 30) = 89.3%
Recall = 250 / (250 + 50) = 83.3%
F-Measure = 2 Ã— (89.3 Ã— 83.3) / (89.3 + 83.3) = 86.2%
```

**Ä°ngilizce Ã–rnek:**
```
1200 emails: 400 work, 800 personal
Work: 350 doÄŸru, 50 yanlÄ±ÅŸ
Personal: 720 doÄŸru, 80 yanlÄ±ÅŸ

TP = 350, FP = 80, TN = 720, FN = 50

Accuracy = (350 + 720) / 1200 = 89.2%
Precision = 350 / (350 + 80) = 81.4%
Recall = 350 / (350 + 50) = 87.5%
F-Measure = 2 Ã— (81.4 Ã— 87.5) / (81.4 + 87.5) = 84.3%
```

#### **âš ï¸ Ortalama Hesaplama:**

**Ham Ortalama:**
```
(SÄ±nÄ±f1_baÅŸarÄ± + SÄ±nÄ±f2_baÅŸarÄ± + ...) / SÄ±nÄ±f_sayÄ±sÄ±
```

**AÄŸÄ±rlÄ±klÄ± Ortalama:**
```
âˆ‘(SÄ±nÄ±f_oranÄ± Ã— BaÅŸarÄ±_oranÄ±)
```

---

**ğŸ“ BAÅARILAR! Slayt konularÄ±nÄ± detaylÄ±ca Ã¶ÄŸrendin! ğŸ“**

*Bu bÃ¶lÃ¼m, slaytlardaki tÃ¼m konularÄ± detaylÄ±ca ele alÄ±r ve pratik uygulamalarÄ± gÃ¶sterir.* 