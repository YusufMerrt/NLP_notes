# ğŸ“ DOÄAL DÄ°L Ä°ÅLEME - DETAYLI KONU ANLATIMI

## ğŸ¯ 1. MARKOV MODELÄ° - DETAYLI ANALÄ°Z

### ğŸ“– **Teorik Temeller**

#### **Tarihsel GeliÅŸim:**
Markov modelleri, **Andrey Markov** tarafÄ±ndan 1906'da geliÅŸtirildi. Ä°lk olarak Rus ÅŸiirindeki sesli harf dizilerini analiz etmek iÃ§in kullanÄ±ldÄ±.

#### **Matematiksel Temel:**
Markov Ã¶zelliÄŸi, bir **stokastik sÃ¼reÃ§**'in gelecekteki durumunun **yalnÄ±zca mevcut duruma** baÄŸlÄ± olduÄŸunu, geÃ§miÅŸ durumlarÄ±n etkisinin olmadÄ±ÄŸÄ±nÄ± belirtir.

**Formal TanÄ±m:**
```
P(Xâ‚™â‚Šâ‚ = x | Xâ‚ = xâ‚, Xâ‚‚ = xâ‚‚, ..., Xâ‚™ = xâ‚™) = P(Xâ‚™â‚Šâ‚ = x | Xâ‚™ = xâ‚™)
```

### ğŸ”¢ **Dil Modellemede Markov Zinciri**

#### **N-Gram Modelleri:**
- **Unigram:** P(wâ‚, wâ‚‚, ..., wâ‚™) = P(wâ‚) Ã— P(wâ‚‚) Ã— ... Ã— P(wâ‚™)
- **Bigram:** P(wâ‚, wâ‚‚, ..., wâ‚™) = P(wâ‚) Ã— P(wâ‚‚|wâ‚) Ã— P(wâ‚ƒ|wâ‚‚) Ã— ... Ã— P(wâ‚™|wâ‚™â‚‹â‚)
- **Trigram:** P(wâ‚, wâ‚‚, ..., wâ‚™) = P(wâ‚) Ã— P(wâ‚‚|wâ‚) Ã— P(wâ‚ƒ|wâ‚,wâ‚‚) Ã— ... Ã— P(wâ‚™|wâ‚™â‚‹â‚‚,wâ‚™â‚‹â‚)

#### **DetaylÄ± Ã–rnek - Bigram Model:**

**Ä°ngilizce Ã–rnek:**

**Verilen Kelime GeÃ§iÅŸ OlasÄ±lÄ±klarÄ±:**
```
P(BAÅLANGIÃ‡|savaÅŸ) = 0.1
P(savaÅŸ|tazminatÄ±) = 0.3
P(tazminatÄ±|aldÄ±lar) = 0.2
P(aldÄ±lar|.) = 0.8
P(.|SON) = 0.9
```

**CÃ¼mle:** "SavaÅŸ tazminatÄ± aldÄ±lar."

**AdÄ±m AdÄ±m Hesaplama:**
```
P(cÃ¼mle) = P(savaÅŸ|BAÅLANGIÃ‡) Ã— P(tazminatÄ±|savaÅŸ) Ã— P(aldÄ±lar|tazminatÄ±) Ã— P(.|aldÄ±lar) Ã— P(SON|.)

P(cÃ¼mle) = 0.1 Ã— 0.3 Ã— 0.2 Ã— 0.8 Ã— 0.9
P(cÃ¼mle) = 0.00432
```

**TÃ¼rkÃ§e Ã–rnek:**

**Verilen Kelime GeÃ§iÅŸ OlasÄ±lÄ±klarÄ±:**
```
P(BAÅLANGIÃ‡|bugÃ¼n) = 0.15
P(bugÃ¼n|hava) = 0.4
P(hava|Ã§ok) = 0.3
P(Ã§ok|gÃ¼zel) = 0.6
P(gÃ¼zel|.) = 0.7
P(.|SON) = 0.9
```

**CÃ¼mle:** "BugÃ¼n hava Ã§ok gÃ¼zel."

**AdÄ±m AdÄ±m Hesaplama:**
```
P(cÃ¼mle) = P(bugÃ¼n|BAÅLANGIÃ‡) Ã— P(hava|bugÃ¼n) Ã— P(Ã§ok|hava) Ã— P(gÃ¼zel|Ã§ok) Ã— P(.|gÃ¼zel) Ã— P(SON|.)

P(cÃ¼mle) = 0.15 Ã— 0.4 Ã— 0.3 Ã— 0.6 Ã— 0.7 Ã— 0.9
P(cÃ¼mle) = 0.006804
```

**KarÅŸÄ±laÅŸtÄ±rmalÄ± Analiz:**

| Ã–zellik | Ä°ngilizce | TÃ¼rkÃ§e |
|---------|-----------|--------|
| **CÃ¼mle** | "SavaÅŸ tazminatÄ± aldÄ±lar." | "BugÃ¼n hava Ã§ok gÃ¼zel." |
| **Uzunluk** | 4 kelime | 5 kelime |
| **OlasÄ±lÄ±k** | 0.00432 | 0.006804 |
| **Konu** | Tarihsel | GÃ¼nlÃ¼k |
| **KarmaÅŸÄ±klÄ±k** | Orta | DÃ¼ÅŸÃ¼k |

### ğŸ“Š **Markov Modelinin AvantajlarÄ± ve DezavantajlarÄ±**

#### âœ… **Avantajlar:**
- **Hesaplama KolaylÄ±ÄŸÄ±:** Sadece mevcut duruma bakÄ±lÄ±r
- **Bellek VerimliliÄŸi:** GeÃ§miÅŸ bilgiler saklanmaz
- **HÄ±zlÄ± EÄŸitim:** Az parametre ile eÄŸitilir

#### âŒ **Dezavantajlar:**
- **KÄ±sa HafÄ±za:** Uzun baÄŸÄ±mlÄ±lÄ±klarÄ± yakalayamaz
- **SÄ±fÄ±r OlasÄ±lÄ±k:** EÄŸitim verisinde gÃ¶rÃ¼lmeyen geÃ§iÅŸler
- **BaÄŸlam EksikliÄŸi:** Uzun mesafeli baÄŸlamlarÄ± unutur

### ğŸ”§ **Smoothing Teknikleri**

#### **Laplace Smoothing (Add-1):**
```
P(wáµ¢|wáµ¢â‚‹â‚) = (count(wáµ¢â‚‹â‚, wáµ¢) + 1) / (count(wáµ¢â‚‹â‚) + V)
```
V = kelime daÄŸarcÄ±ÄŸÄ± boyutu

#### **Interpolasyon:**
```
P(wáµ¢|wáµ¢â‚‹â‚‚, wáµ¢â‚‹â‚) = Î»â‚P(wáµ¢) + Î»â‚‚P(wáµ¢|wáµ¢â‚‹â‚) + Î»â‚ƒP(wáµ¢|wáµ¢â‚‹â‚‚, wáµ¢â‚‹â‚)
```

---

## ğŸ¯ 2. SHIFT-REDUCE ALGORÄ°TMASI - DETAYLI ANALÄ°Z

### ğŸ“– **Teorik Temeller**

#### **Tarihsel GeliÅŸim:**
Shift-Reduce algoritmasÄ±, **Donald Knuth** tarafÄ±ndan 1965'te geliÅŸtirilen **LR parsing** algoritmasÄ±nÄ±n temelidir. **Bottom-up parsing** yaklaÅŸÄ±mÄ±nÄ± kullanÄ±r.

#### **Matematiksel Temel:**
Shift-Reduce, **yÄ±ÄŸÄ±n (stack)** tabanlÄ± bir algoritmadÄ±r. CÃ¼mleyi **soldan saÄŸa** iÅŸler ve **en saÄŸdan tÃ¼retme (rightmost derivation)** yapar.

**Temel Ä°ÅŸlemler:**
1. **Shift:** Girdi kelimesini yÄ±ÄŸÄ±na ekle
2. **Reduce:** YÄ±ÄŸÄ±ndaki Ã¶ÄŸeleri gramer kuralÄ±na gÃ¶re birleÅŸtir
3. **Accept:** CÃ¼mle baÅŸarÄ±yla parse edildi
4. **Error:** Gramer hatasÄ± tespit edildi

### ğŸ”¢ **Algoritma AdÄ±mlarÄ± - DetaylÄ± AÃ§Ä±klama**

#### **AdÄ±m 1: YÄ±ÄŸÄ±n ve Girdi HazÄ±rlama**
```
YÄ±ÄŸÄ±n: [] (boÅŸ)
Girdi: [wâ‚, wâ‚‚, wâ‚ƒ, ..., wâ‚™]
```

#### **AdÄ±m 2: Ä°ÅŸlem DÃ¶ngÃ¼sÃ¼**
Her adÄ±mda ÅŸu kararlardan biri alÄ±nÄ±r:
- **Shift:** Girdiden kelime al, yÄ±ÄŸÄ±na ekle
- **Reduce:** YÄ±ÄŸÄ±ndaki Ã¶ÄŸeleri gramer kuralÄ±na gÃ¶re birleÅŸtir
- **Accept:** CÃ¼mle tamamlandÄ±
- **Error:** Gramer hatasÄ±

### ğŸ“ **DetaylÄ± Ã–rnek - AdÄ±m AdÄ±m**

#### **Ä°ngilizce Ã–rnek:**

**Gramer:**
```
S â†’ NP VP
NP â†’ Det N
VP â†’ V NP
Det â†’ the
N â†’ cat, dog
V â†’ chased
```

**CÃ¼mle:** "the cat chased the dog"

**AdÄ±m AdÄ±m Ä°ÅŸlem:**

| AdÄ±m | YÄ±ÄŸÄ±n | Girdi | Ä°ÅŸlem | AÃ§Ä±klama |
|------|-------|-------|-------|----------|
| 1 | [] | [the, cat, chased, the, dog] | Shift | "the" yÄ±ÄŸÄ±na ekle |
| 2 | [the] | [cat, chased, the, dog] | Shift | "cat" yÄ±ÄŸÄ±na ekle |
| 3 | [the, cat] | [chased, the, dog] | Reduce | Det + N â†’ NP |
| 4 | [NP] | [chased, the, dog] | Shift | "chased" yÄ±ÄŸÄ±na ekle |
| 5 | [NP, chased] | [the, dog] | Shift | "the" yÄ±ÄŸÄ±na ekle |
| 6 | [NP, chased, the] | [dog] | Shift | "dog" yÄ±ÄŸÄ±na ekle |
| 7 | [NP, chased, the, dog] | [] | Reduce | Det + N â†’ NP |
| 8 | [NP, chased, NP] | [] | Reduce | V + NP â†’ VP |
| 9 | [NP, VP] | [] | Reduce | NP + VP â†’ S |
| 10 | [S] | [] | Accept | CÃ¼mle tamamlandÄ± |

#### **TÃ¼rkÃ§e Ã–rnek:**

**Gramer:**
```
S â†’ NP VP
NP â†’ Det N
VP â†’ V NP
Det â†’ bu, ÅŸu, o
N â†’ kedi, kÃ¶pek, kuÅŸ
V â†’ kovaladÄ±, gÃ¶rdÃ¼
```

**CÃ¼mle:** "bu kedi kÃ¶peÄŸi kovaladÄ±"

**AdÄ±m AdÄ±m Ä°ÅŸlem:**

| AdÄ±m | YÄ±ÄŸÄ±n | Girdi | Ä°ÅŸlem | AÃ§Ä±klama |
|------|-------|-------|-------|----------|
| 1 | [] | [bu, kedi, kÃ¶peÄŸi, kovaladÄ±] | Shift | "bu" yÄ±ÄŸÄ±na ekle |
| 2 | [bu] | [kedi, kÃ¶peÄŸi, kovaladÄ±] | Shift | "kedi" yÄ±ÄŸÄ±na ekle |
| 3 | [bu, kedi] | [kÃ¶peÄŸi, kovaladÄ±] | Reduce | Det + N â†’ NP |
| 4 | [NP] | [kÃ¶peÄŸi, kovaladÄ±] | Shift | "kÃ¶peÄŸi" yÄ±ÄŸÄ±na ekle |
| 5 | [NP, kÃ¶peÄŸi] | [kovaladÄ±] | Shift | "kovaladÄ±" yÄ±ÄŸÄ±na ekle |
| 6 | [NP, kÃ¶peÄŸi, kovaladÄ±] | [] | Reduce | N + V â†’ VP |
| 7 | [NP, VP] | [] | Reduce | NP + VP â†’ S |
| 8 | [S] | [] | Accept | CÃ¼mle tamamlandÄ± |

#### **KarÅŸÄ±laÅŸtÄ±rmalÄ± Analiz:**

| Ã–zellik | Ä°ngilizce | TÃ¼rkÃ§e |
|---------|-----------|--------|
| **CÃ¼mle** | "the cat chased the dog" | "bu kedi kÃ¶peÄŸi kovaladÄ±" |
| **Uzunluk** | 5 kelime | 4 kelime |
| **AdÄ±m SayÄ±sÄ±** | 10 adÄ±m | 8 adÄ±m |
| **YapÄ±** | Det N V Det N | Det N N V |
| **KarmaÅŸÄ±klÄ±k** | Orta | DÃ¼ÅŸÃ¼k |
| **Reduce Ä°ÅŸlemleri** | 3 adet | 2 adet |

### ğŸ“Š **Shift-Reduce AlgoritmasÄ±nÄ±n AvantajlarÄ± ve DezavantajlarÄ±**

#### âœ… **Avantajlar:**
- **HÄ±zlÄ± Ä°ÅŸlem:** O(n) zaman karmaÅŸÄ±klÄ±ÄŸÄ±
- **Bellek Verimli:** Sadece yÄ±ÄŸÄ±n boyutu kadar bellek
- **GerÃ§ek ZamanlÄ±:** Kelime kelime iÅŸleyebilir
- **Hata Tespiti:** Ä°lk hatada durur
- **Esnek:** Herhangi bir gramer tÃ¼rÃ¼ ile Ã§alÄ±ÅŸÄ±r

#### âŒ **Dezavantajlar:**
- **KarmaÅŸÄ±k Karar:** Shift mi Reduce mu yapÄ±lacaÄŸÄ±na karar vermek zor
- **Ambiguity:** Birden fazla parse aÄŸacÄ± olabilir
- **Gramer BaÄŸÄ±mlÄ±lÄ±ÄŸÄ±:** Gramer kalitesi Ã¶nemli
- **Hata Kurtarma:** Hata sonrasÄ± devam etmek zor

### ğŸ”§ **Karar MekanizmasÄ±**

#### **Shift-Reduce KararÄ±:**
Algoritma her adÄ±mda ÅŸu kriterlere bakar:
1. **YÄ±ÄŸÄ±ndaki son Ã¶ÄŸeler** gramer kuralÄ±na uyuyor mu?
2. **Girdideki sonraki kelime** hangi kuralÄ± bekliyor?
3. **Lookahead:** Bir sonraki kelimeye bakarak karar ver

#### **Ã–rnek Karar SÃ¼reci:**
```
YÄ±ÄŸÄ±n: [NP, chased]
Girdi: [the, dog]
Durum: chased (V) var, NP bekliyor
Karar: Shift (Ã§Ã¼nkÃ¼ NP oluÅŸturmak iÃ§in daha fazla kelime gerekli)
```

### âš¡ **Algoritma KarmaÅŸÄ±klÄ±ÄŸÄ±**

#### **Zaman KarmaÅŸÄ±klÄ±ÄŸÄ±:**
- **O(n)** - Her kelime bir kez iÅŸlenir
- **En kÃ¶tÃ¼ durum:** O(nÂ²) - Her adÄ±mda reduce

#### **Uzay KarmaÅŸÄ±klÄ±ÄŸÄ±:**
- **O(n)** - YÄ±ÄŸÄ±n boyutu cÃ¼mle uzunluÄŸu kadar

#### **Pratik Performans:**
```
n = 20 kelimelik cÃ¼mle iÃ§in:
- Zaman: ~20-40 iÅŸlem
- Uzay: ~20 kelime
```

---

## ğŸ¯ 3. CYK ALGORÄ°TMASI - DETAYLI ANALÄ°Z

### ğŸ“– **Teorik Temeller**

#### **Tarihsel GeliÅŸim:**
CYK (Cocke-Younger-Kasami) algoritmasÄ±, **John Cocke**, **Daniel Younger** ve **Tadao Kasami** tarafÄ±ndan 1960'larda baÄŸÄ±msÄ±z olarak geliÅŸtirildi.

#### **Matematiksel Temel:**
CYK, **Chomsky Normal Form** (CNF) gramerlerle Ã§alÄ±ÅŸan **dinamik programlama** algoritmasÄ±dÄ±r.

**CNF KurallarÄ±:**
1. A â†’ BC (iki terminal olmayan)
2. A â†’ a (tek terminal)
3. S â†’ Îµ (sadece baÅŸlangÄ±Ã§ sembolÃ¼ iÃ§in)

### ğŸ”¢ **Algoritma AdÄ±mlarÄ± - DetaylÄ± AÃ§Ä±klama**

#### **AdÄ±m 1: Grameri CNF'ye DÃ¶nÃ¼ÅŸtÃ¼r**
```
Orijinal Gramer:
S â†’ NP VP
NP â†’ Det N | N
VP â†’ V NP
Det â†’ the
N â†’ cat, dog, mouse
V â†’ chased, saw

CNF DÃ¶nÃ¼ÅŸÃ¼mÃ¼:
S â†’ NP VP
NP â†’ Det N
NP â†’ N
VP â†’ V NP
Det â†’ the
N â†’ cat
N â†’ dog  
N â†’ mouse
V â†’ chased
V â†’ saw
```

#### **AdÄ±m 2: Tablo OluÅŸtur**
nÃ—n matris oluÅŸtur (n = cÃ¼mle uzunluÄŸu)

#### **AdÄ±m 3: KÃ¶ÅŸegen Doldur (Tek Kelimeler)**
```
CÃ¼mle: "the cat chased the dog"
Uzunluk: 5

    1     2     3     4     5
1  Det   -     -     -     -
2   -    N     -     -     -
3   -    -     V     -     -
4   -    -     -    Det    -
5   -    -     -     -     N
```

#### **AdÄ±m 4: Ãœst KÃ¶ÅŸegenleri Doldur**

**2'li Gruplar (Uzunluk 2):**
```
[1,2]: Det + N = NP
[2,3]: N + V = (geÃ§ersiz)
[3,4]: V + Det = (geÃ§ersiz)
[4,5]: Det + N = NP

    1     2     3     4     5
1  Det   NP    -     -     -
2   -    N     -     -     -
3   -    -     V     -     -
4   -    -     -    Det    NP
5   -    -     -     -     N
```

**3'lÃ¼ Gruplar (Uzunluk 3):**
```
[1,3]: Det + N + V = (geÃ§ersiz)
[2,4]: N + V + Det = (geÃ§ersiz)
[3,5]: V + Det + N = VP

    1     2     3     4     5
1  Det   NP    -     -     -
2   -    N     -     -     -
3   -    -     V     -     -
4   -    -     -    Det    NP
5   -    -     -     -     N
```

**4'lÃ¼ Gruplar (Uzunluk 4):**
```
[1,4]: Det + N + V + Det = (geÃ§ersiz)
[2,5]: N + V + Det + N = (geÃ§ersiz)
```

**5'li Gruplar (Uzunluk 5):**
```
[1,5]: Det + N + V + Det + N = S

    1     2     3     4     5
1  Det   NP    -     -     S
2   -    N     -     -     -
3   -    -     V     -     -
4   -    -     -    Det    NP
5   -    -     -     -     N
```

### ğŸ“ **DetaylÄ± Ã–rnek - AdÄ±m AdÄ±m**

#### **Ä°ngilizce Ã–rnek:**

**Gramer:**
```
S â†’ NP VP
NP â†’ Det N
VP â†’ V NP
Det â†’ the
N â†’ cat, dog
V â†’ chased
```

**CÃ¼mle:** "the cat chased the dog"

**Tablo Doldurma SÃ¼reci:**

1. **KÃ¶ÅŸegen (Tek Kelimeler):**
   - [1,1]: "the" â†’ Det
   - [2,2]: "cat" â†’ N
   - [3,3]: "chased" â†’ V
   - [4,4]: "the" â†’ Det
   - [5,5]: "dog" â†’ N

2. **2'li Gruplar:**
   - [1,2]: Det + N â†’ NP
   - [4,5]: Det + N â†’ NP

3. **3'lÃ¼ Gruplar:**
   - [3,5]: V + NP â†’ VP

4. **5'li Grup:**
   - [1,5]: NP + VP â†’ S

**SonuÃ§:** S kÃ¶ÅŸesinde S varsa cÃ¼mle gramer tarafÄ±ndan Ã¼retilir.

#### **TÃ¼rkÃ§e Ã–rnek:**

**Gramer:**
```
S â†’ NP VP
NP â†’ Det N
VP â†’ V NP
Det â†’ bu, ÅŸu, o
N â†’ kedi, kÃ¶pek, kuÅŸ
V â†’ kovaladÄ±, gÃ¶rdÃ¼
```

**CÃ¼mle:** "bu kedi kÃ¶peÄŸi kovaladÄ±"

**Tablo Doldurma SÃ¼reci:**

1. **KÃ¶ÅŸegen (Tek Kelimeler):**
   - [1,1]: "bu" â†’ Det
   - [2,2]: "kedi" â†’ N
   - [3,3]: "kÃ¶peÄŸi" â†’ N
   - [4,4]: "kovaladÄ±" â†’ V

2. **2'li Gruplar:**
   - [1,2]: Det + N â†’ NP
   - [3,4]: N + V â†’ VP

3. **4'lÃ¼ Grup:**
   - [1,4]: NP + VP â†’ S

**SonuÃ§:** S kÃ¶ÅŸesinde S varsa cÃ¼mle gramer tarafÄ±ndan Ã¼retilir.

#### **KarÅŸÄ±laÅŸtÄ±rmalÄ± Tablo:**

| Ã–zellik | Ä°ngilizce | TÃ¼rkÃ§e |
|---------|-----------|--------|
| **CÃ¼mle** | "the cat chased the dog" | "bu kedi kÃ¶peÄŸi kovaladÄ±" |
| **Uzunluk** | 5 kelime | 4 kelime |
| **YapÄ±** | Det N V Det N | Det N N V |
| **KarmaÅŸÄ±klÄ±k** | 5Ã—5 tablo | 4Ã—4 tablo |
| **Ä°ÅŸlem SayÄ±sÄ±** | 10 adÄ±m | 8 adÄ±m |

### âš¡ **Algoritma KarmaÅŸÄ±klÄ±ÄŸÄ±**

#### **Zaman KarmaÅŸÄ±klÄ±ÄŸÄ±:**
- **O(nÂ³)** - ÃœÃ§ iÃ§ iÃ§e dÃ¶ngÃ¼
- **Uzay KarmaÅŸÄ±klÄ±ÄŸÄ±:** O(nÂ²) - nÃ—n tablo

#### **Pratik Uygulamalar:**
```
n = 10 kelimelik cÃ¼mle iÃ§in:
- Zaman: 10Â³ = 1000 iÅŸlem
- Uzay: 10Â² = 100 hÃ¼cre
```

---

## ğŸ¯ 4. SHIFT-REDUCE vs CYK - DETAYLI KARÅILAÅTIRMA

### ğŸ“Š **Temel KarÅŸÄ±laÅŸtÄ±rma Tablosu**

| Ã–zellik | Shift-Reduce | CYK |
|---------|--------------|-----|
| **YÃ¶ntem** | YÄ±ÄŸÄ±n tabanlÄ± | Tablo doldurma |
| **Parsing TÃ¼rÃ¼** | Bottom-up | Bottom-up |
| **Gramer TÃ¼rÃ¼** | Herhangi | CNF |
| **Zaman KarmaÅŸÄ±klÄ±ÄŸÄ±** | O(n) | O(nÂ³) |
| **Uzay KarmaÅŸÄ±klÄ±ÄŸÄ±** | O(n) | O(nÂ²) |
| **GerÃ§ek ZamanlÄ±** | âœ… Evet | âŒ HayÄ±r |
| **Hata Tespiti** | HÄ±zlÄ± | YavaÅŸ |
| **Ambiguity** | Ä°lk bulduÄŸunu alÄ±r | TÃ¼m parse aÄŸaÃ§larÄ±nÄ± bulur |
| **Uygulama** | Pratik | Teorik |
| **Bellek KullanÄ±mÄ±** | DÃ¼ÅŸÃ¼k | YÃ¼ksek |

### ğŸ”„ **DetaylÄ± KarÅŸÄ±laÅŸtÄ±rma**

#### **1. Ã‡alÄ±ÅŸma Prensibi:**

**Shift-Reduce:**
```
- Soldan saÄŸa iÅŸler
- YÄ±ÄŸÄ±n kullanÄ±r
- Her adÄ±mda karar verir
- Ä°lk hatada durur
```

**CYK:**
```
- TÃ¼m cÃ¼mleyi gÃ¶rÃ¼r
- Tablo doldurur
- Sonunda karar verir
- TÃ¼m olasÄ±lÄ±klarÄ± hesaplar
```

#### **2. Gramer Gereksinimleri:**

**Shift-Reduce:**
```
- Herhangi bir gramer tÃ¼rÃ¼
- DÃ¶nÃ¼ÅŸtÃ¼rme gerekmez
- Esnek yapÄ±
```

**CYK:**
```
- Sadece CNF gramerler
- DÃ¶nÃ¼ÅŸtÃ¼rme zorunlu
- KÄ±sÄ±tlÄ± yapÄ±
```

#### **3. Hata YÃ¶netimi:**

**Shift-Reduce:**
```
- Ä°lk hatada durur
- HÄ±zlÄ± hata tespiti
- Hata kurtarma zor
```

**CYK:**
```
- TÃ¼m tabloyu doldurur
- YavaÅŸ hata tespiti
- Hata analizi kolay
```

### ğŸ“ **Pratik Ã–rneklerle KarÅŸÄ±laÅŸtÄ±rma**

#### **Ã–rnek 1: "the cat chased the dog"**

**Shift-Reduce SÃ¼reci:**
```
AdÄ±m 1: [the] â† Shift
AdÄ±m 2: [the, cat] â† Shift  
AdÄ±m 3: [NP] â† Reduce (Det + N â†’ NP)
AdÄ±m 4: [NP, chased] â† Shift
AdÄ±m 5: [NP, chased, the] â† Shift
AdÄ±m 6: [NP, chased, the, dog] â† Shift
AdÄ±m 7: [NP, chased, NP] â† Reduce (Det + N â†’ NP)
AdÄ±m 8: [NP, VP] â† Reduce (V + NP â†’ VP)
AdÄ±m 9: [S] â† Reduce (NP + VP â†’ S)
AdÄ±m 10: Accept
```

**CYK SÃ¼reci:**
```
1. Tablo oluÅŸtur (5Ã—5)
2. KÃ¶ÅŸegen doldur: [Det, N, V, Det, N]
3. 2'li gruplar: [NP, -, -, NP]
4. 3'lÃ¼ gruplar: [-, -, VP, -]
5. 5'li grup: [S]
6. SonuÃ§: S kÃ¶ÅŸesinde S var â†’ Kabul
```

#### **Ã–rnek 2: Ambiguous CÃ¼mle**

**CÃ¼mle:** "I saw the man with the telescope"

**Shift-Reduce:**
```
- Ä°lk bulduÄŸu parse aÄŸacÄ±nÄ± kabul eder
- HÄ±zlÄ± karar verir
- DiÄŸer olasÄ±lÄ±klarÄ± gÃ¶rmez
```

**CYK:**
```
- TÃ¼m olasÄ± parse aÄŸaÃ§larÄ±nÄ± bulur
- Ambiguity'yi tespit eder
- Hangi yapÄ±nÄ±n daha olasÄ± olduÄŸunu hesaplar
```

### âš¡ **Performans KarÅŸÄ±laÅŸtÄ±rmasÄ±**

#### **Zaman KarÅŸÄ±laÅŸtÄ±rmasÄ±:**
```
CÃ¼mle UzunluÄŸu | Shift-Reduce | CYK
10 kelime      | ~10-20 iÅŸlem | ~1000 iÅŸlem
20 kelime      | ~20-40 iÅŸlem | ~8000 iÅŸlem
50 kelime      | ~50-100 iÅŸlem| ~125000 iÅŸlem
```

#### **Bellek KarÅŸÄ±laÅŸtÄ±rmasÄ±:**
```
CÃ¼mle UzunluÄŸu | Shift-Reduce | CYK
10 kelime      | ~10 kelime   | ~100 hÃ¼cre
20 kelime      | ~20 kelime   | ~400 hÃ¼cre
50 kelime      | ~50 kelime   | ~2500 hÃ¼cre
```

### ğŸ¯ **Hangi Durumda Hangi Algoritma?**

#### **Shift-Reduce Kullan:**
- **GerÃ§ek zamanlÄ±** uygulamalar
- **HÄ±zlÄ±** parsing gerektiÄŸinde
- **Bellek** kÄ±sÄ±tlÄ± olduÄŸunda
- **Pratik** uygulamalar
- **Hata tespiti** Ã¶nemli olduÄŸunda

#### **CYK Kullan:**
- **Teorik** analiz
- **Ambiguity** tespiti
- **TÃ¼m olasÄ±lÄ±klarÄ±** gÃ¶rmek istediÄŸinde
- **Gramer doÄŸrulama**
- **Akademik** Ã§alÄ±ÅŸmalar

### ğŸ“Š **Gramer DÃ¶nÃ¼ÅŸtÃ¼rme Ã–rneÄŸi**

#### **Normal Gramer â†’ CNF:**

**Orijinal Gramer:**
```
S â†’ NP VP
NP â†’ Det N | N
VP â†’ V NP | V NP PP
PP â†’ P NP
Det â†’ the, a
N â†’ cat, dog, telescope
V â†’ saw, chased
P â†’ with
```

**CNF DÃ¶nÃ¼ÅŸÃ¼mÃ¼:**
```
S â†’ NP VP
NP â†’ Det N
NP â†’ N
VP â†’ V NP
VP â†’ V NP_PP
NP_PP â†’ NP PP
PP â†’ P NP
Det â†’ the
Det â†’ a
N â†’ cat
N â†’ dog
N â†’ telescope
V â†’ saw
V â†’ chased
P â†’ with
```

**DÃ¶nÃ¼ÅŸtÃ¼rme KurallarÄ±:**
1. **A â†’ BCD** â†’ **A â†’ BE, E â†’ CD**
2. **A â†’ a|b** â†’ **A â†’ a, A â†’ b**
3. **Terminal olmayanlar** sadece 2 terminal olmayan Ã¼retebilir

---

## ğŸ¯ 5. EXPECTATION (BEKLENTÄ°) - DETAYLI ANALÄ°Z

### ğŸ“– **Teorik Temeller**

#### **Matematiksel TanÄ±m:**
Beklenti, bir rastgele deÄŸiÅŸkenin **ortalama deÄŸeri**'dir.

**Formal TanÄ±m:**
```
E[X] = âˆ‘ xáµ¢ Ã— P(xáµ¢)
```

#### **SÃ¼rekli DeÄŸiÅŸkenler Ä°Ã§in:**
```
E[X] = âˆ« x Ã— f(x) dx
```

### ğŸ”¢ **Markov Modelinde Beklenti**

#### **Durum GeÃ§iÅŸ Beklentisi:**
Bir durumdan diÄŸerine geÃ§iÅŸin beklenen sayÄ±sÄ±.

**FormÃ¼l:**
```
E[GeÃ§iÅŸ] = âˆ‘ P(geÃ§iÅŸáµ¢) Ã— deÄŸeráµ¢
```

#### **DetaylÄ± Ã–rnek - Hava Durumu Modeli:**

**GeÃ§iÅŸ OlasÄ±lÄ±klarÄ±:**
```
GÃ¼neÅŸli â†’ YaÄŸmurlu: 0.3
GÃ¼neÅŸli â†’ Bulutlu: 0.5
GÃ¼neÅŸli â†’ GÃ¼neÅŸli: 0.2
```

**Beklenti Hesaplama:**
```
E[GeÃ§iÅŸ] = 0.3 Ã— 1 + 0.5 Ã— 1 + 0.2 Ã— 1 = 1
```

**AÃ§Ä±klama:** Her geÃ§iÅŸ 1 adÄ±m sayÄ±ldÄ±ÄŸÄ± iÃ§in toplam beklenti 1'dir.

### ğŸ“Š **Dil Modellemede Beklenti**

#### **Kelime Beklentisi:**
Bir kelimenin bir cÃ¼mlede kaÃ§ kez geÃ§eceÄŸinin beklenen deÄŸeri.

**Ã–rnek:**
```
P("the") = 0.05
CÃ¼mle uzunluÄŸu = 20

E["the"] = 20 Ã— 0.05 = 1
```

#### **CÃ¼mle UzunluÄŸu Beklentisi:**
Bir dil modelinin Ã¼reteceÄŸi cÃ¼mlelerin ortalama uzunluÄŸu.

**FormÃ¼l:**
```
E[Uzunluk] = âˆ‘ i Ã— P(Uzunluk = i)
```

### ğŸ”§ **KoÅŸullu Beklenti**

#### **TanÄ±m:**
Bir olayÄ±n gerÃ§ekleÅŸmesi koÅŸuluyla baÅŸka bir olayÄ±n beklenen deÄŸeri.

**FormÃ¼l:**
```
E[X|Y] = âˆ‘ x Ã— P(X=x|Y)
```

#### **Markov Modelinde:**
```
E[SonrakiKelime|MevcutKelime] = âˆ‘ kelime Ã— P(SonrakiKelime=kelime|MevcutKelime)
```

### ğŸ“ **Pratik Uygulamalar**

#### **1. Metin Ãœretimi:**
```
Verilen: "BugÃ¼n hava"
Beklenti: E[SonrakiKelime] = 0.3Ã—"gÃ¼zel" + 0.4Ã—"kÃ¶tÃ¼" + 0.3Ã—"yaÄŸmurlu"
SonuÃ§: "kÃ¶tÃ¼" (en yÃ¼ksek olasÄ±lÄ±k)
```

#### **2. Model DeÄŸerlendirmesi:**
```
Test cÃ¼mlesi: "the cat sat on the mat"
Model tahminleri:
- P("cat"|"the") = 0.1
- P("sat"|"cat") = 0.05
- P("on"|"sat") = 0.2
- P("the"|"on") = 0.3
- P("mat"|"the") = 0.15

Beklenti: E[OlasÄ±lÄ±k] = 0.1 Ã— 0.05 Ã— 0.2 Ã— 0.3 Ã— 0.15 = 0.000045
```

### âš¡ **Beklenti Hesaplama Teknikleri**

#### **1. Monte Carlo SimÃ¼lasyonu:**
```
1. Ã‡ok sayÄ±da rastgele Ã¶rnek Ã¼ret
2. Her Ã¶rneÄŸin deÄŸerini hesapla
3. OrtalamasÄ±nÄ± al
```

#### **2. Dinamik Programlama:**
```
1. Alt problemleri Ã§Ã¶z
2. SonuÃ§larÄ± tabloda sakla
3. BÃ¼yÃ¼k problemi kÃ¼Ã§Ã¼k parÃ§alardan oluÅŸtur
```

### ğŸ“Š **Beklenti vs Varyans**

#### **Beklenti (E[X]):**
- Ortalama deÄŸer
- Merkezi eÄŸilim

#### **Varyans (Var[X]):**
```
Var[X] = E[(X - E[X])Â²] = E[XÂ²] - (E[X])Â²
```
- DeÄŸiÅŸkenlik Ã¶lÃ§Ã¼sÃ¼
- DaÄŸÄ±lÄ±mÄ±n geniÅŸliÄŸi

### ğŸ”„ **Markov Modelinde Varyans**

#### **Durum GeÃ§iÅŸ VaryansÄ±:**
```
Var[GeÃ§iÅŸ] = E[GeÃ§iÅŸÂ²] - (E[GeÃ§iÅŸ])Â²
```

**Ã–rnek:**
```
GeÃ§iÅŸ deÄŸerleri: [1, 1, 1] (her geÃ§iÅŸ 1 adÄ±m)
OlasÄ±lÄ±klar: [0.3, 0.5, 0.2]

E[GeÃ§iÅŸ] = 1
E[GeÃ§iÅŸÂ²] = 0.3Ã—1Â² + 0.5Ã—1Â² + 0.2Ã—1Â² = 1
Var[GeÃ§iÅŸ] = 1 - 1Â² = 0
```

---

## ğŸ¯ SINAVDA Ã‡IKABÄ°LECEK SORU TÄ°PLERÄ°

### ğŸ“ **Markov Modeli SorularÄ±:**

#### **Soru Tipi 1: CÃ¼mle OlasÄ±lÄ±ÄŸÄ± Hesaplama**
```
Verilen: GeÃ§iÅŸ olasÄ±lÄ±klarÄ± ve bir cÃ¼mle
Ä°stenen: CÃ¼mlenin olasÄ±lÄ±ÄŸÄ±nÄ± hesapla

Ã‡Ã¶zÃ¼m: P(wâ‚...wâ‚™) = P(wâ‚) Ã— âˆP(wáµ¢|wáµ¢â‚‹â‚)
```

#### **Soru Tipi 2: Beklenti Hesaplama**
```
Verilen: Durum geÃ§iÅŸ olasÄ±lÄ±klarÄ±
Ä°stenen: Beklenen geÃ§iÅŸ sayÄ±sÄ±nÄ± hesapla

Ã‡Ã¶zÃ¼m: E[X] = âˆ‘xáµ¢ Ã— P(xáµ¢)
```

### ğŸ“ **CYK AlgoritmasÄ± SorularÄ±:**

#### **Soru Tipi 1: Tablo Doldurma**
```
Verilen: CNF gramer ve cÃ¼mle
Ä°stenen: CYK tablosunu doldur

Ã‡Ã¶zÃ¼m: KÃ¶ÅŸegenden baÅŸla, Ã¼st kÃ¶ÅŸegenleri doldur
```

#### **Soru Tipi 2: Gramer DÃ¶nÃ¼ÅŸtÃ¼rme**
```
Verilen: Normal gramer
Ä°stenen: CNF'ye dÃ¶nÃ¼ÅŸtÃ¼r

Ã‡Ã¶zÃ¼m: A â†’ BCD â†’ A â†’ BE, E â†’ CD
```

### ğŸ“ **Shift-Reduce SorularÄ±:**

#### **Soru Tipi 1: AdÄ±m AdÄ±m Parsing**
```
Verilen: Gramer ve cÃ¼mle
Ä°stenen: Shift-Reduce adÄ±mlarÄ±nÄ± gÃ¶ster

Ã‡Ã¶zÃ¼m: YÄ±ÄŸÄ±n ve girdi durumlarÄ±nÄ± takip et
```

#### **Soru Tipi 2: Karar MekanizmasÄ±**
```
Verilen: YÄ±ÄŸÄ±n ve girdi durumu
Ä°stenen: Shift mi Reduce mu yapÄ±lacaÄŸÄ±na karar ver

Ã‡Ã¶zÃ¼m: Gramer kurallarÄ±na ve lookahead'e bak
```

### ğŸ“ **Expectation SorularÄ±:**

#### **Soru Tipi 1: KoÅŸullu Beklenti**
```
Verilen: KoÅŸullu olasÄ±lÄ±klar
Ä°stenen: E[X|Y] hesapla

Ã‡Ã¶zÃ¼m: E[X|Y] = âˆ‘x Ã— P(X=x|Y)
```

#### **Soru Tipi 2: Model DeÄŸerlendirmesi**
```
Verilen: Test verisi ve model tahminleri
Ä°stenen: Beklenen performansÄ± hesapla

Ã‡Ã¶zÃ¼m: E[Performans] = âˆ‘P(tahmin) Ã— deÄŸer
```

---

## ğŸ“ BAÅARI TAVSÄ°YELERÄ°

### âœ… **Son GÃ¼n Ã‡alÄ±ÅŸma PlanÄ±:**

1. **Markov Modeli (2 saat):**
   - FormÃ¼lleri ezberle
   - Ã–rnek problemler Ã§Ã¶z
   - Beklenti hesaplamalarÄ± yap

2. **CYK AlgoritmasÄ± (2 saat):**
   - Tablo doldurma pratiÄŸi
   - CNF dÃ¶nÃ¼ÅŸÃ¼mleri
   - Shift-Reduce karÅŸÄ±laÅŸtÄ±rmasÄ±

3. **Shift-Reduce (1 saat):**
   - AdÄ±m adÄ±m parsing
   - Karar mekanizmasÄ±
   - Pratik Ã¶rnekler

4. **Expectation (1 saat):**
   - KoÅŸullu beklenti hesaplamalarÄ±
   - Varyans formÃ¼lleri
   - Pratik uygulamalar

### ğŸ¯ **SÄ±nav Ä°puÃ§larÄ±:**

- **Zaman yÃ¶netimi:** CYK en Ã§ok zaman alÄ±r
- **Hesaplama kontrolÃ¼:** SonuÃ§larÄ± mantÄ±k kontrolÃ¼nden geÃ§ir
- **Birim kontrolÃ¼:** OlasÄ±lÄ±klar 0-1 arasÄ±nda olmalÄ±
- **Grafik Ã§iz:** CYK iÃ§in tablo Ã§iz
- **AdÄ±m takibi:** Shift-Reduce iÃ§in yÄ±ÄŸÄ±n durumunu takip et

### ğŸ“ **FormÃ¼l KartÄ±:**
```
Markov: P(wâ‚...wâ‚™) = P(wâ‚) Ã— âˆP(wáµ¢|wáµ¢â‚‹â‚)
CYK: O(nÂ³) zaman, O(nÂ²) uzay
Shift-Reduce: O(n) zaman, O(n) uzay
Expectation: E[X] = âˆ‘xáµ¢ Ã— P(xáµ¢)
KoÅŸullu: E[X|Y] = âˆ‘x Ã— P(X=x|Y)
Varyans: Var[X] = E[XÂ²] - (E[X])Â²
```

---

**ğŸ“ BAÅARILAR! Final sÄ±navÄ±nda baÅŸarÄ±lar dilerim! ğŸ“**

*Bu dÃ¶kÃ¼man, CYK, Markov, Shift-Reduce ve Expectation konularÄ±na Ã¶zel olarak detaylandÄ±rÄ±lmÄ±ÅŸtÄ±r.*

### ğŸ“ **Pratik Ã–rnek - Spam Tespiti**

**Verilen:**
- 1000 e-posta test edildi
- 200 spam, 800 normal
- SonuÃ§lar:
  - 150 spam doÄŸru tespit edildi
  - 50 spam yanlÄ±ÅŸ tespit edildi (normal sanÄ±ldÄ±)
  - 100 normal yanlÄ±ÅŸ tespit edildi (spam sanÄ±ldÄ±)
  - 700 normal doÄŸru tespit edildi

**KarmaÅŸÄ±klÄ±k Matrisi:**
```
                Tahmin
GerÃ§ek    Spam    Normal
Spam      150     50
Normal    100     700
```

**Hesaplamalar:**
- TP = 150 (Spam doÄŸru tespit)
- FP = 100 (Normal yanlÄ±ÅŸ spam)
- TN = 700 (Normal doÄŸru tespit)
- FN = 50 (Spam yanlÄ±ÅŸ normal)

---

### ğŸ“ **TÃ¼rkÃ§e Ã–rnek - Duygu Analizi**

**Verilen:**
- 500 yorum test edildi
- 300 pozitif, 200 negatif
- SonuÃ§lar:
  - 250 pozitif doÄŸru tespit edildi
  - 50 pozitif yanlÄ±ÅŸ tespit edildi (negatif sanÄ±ldÄ±)
  - 30 negatif yanlÄ±ÅŸ tespit edildi (pozitif sanÄ±ldÄ±)
  - 170 negatif doÄŸru tespit edildi

**KarmaÅŸÄ±klÄ±k Matrisi:**
```
                Tahmin
GerÃ§ek    Pozitif    Negatif
Pozitif   250        50
Negatif   30         170
```

**Hesaplamalar:**
- TP = 250 (Pozitif doÄŸru tespit)
- FP = 30 (Negatif yanlÄ±ÅŸ pozitif)
- TN = 170 (Negatif doÄŸru tespit)
- FN = 50 (Pozitif yanlÄ±ÅŸ negatif)

---

### ğŸ“ **Ä°ngilizce Ã–rnek - Email Classification**

**Given:**
- 1200 emails tested
- 400 work, 800 personal
- Results:
  - 350 work emails correctly classified
  - 50 work emails misclassified as personal
  - 80 personal emails misclassified as work
  - 720 personal emails correctly classified

**Confusion Matrix:**
```
                Prediction
Actual      Work    Personal
Work        350     50
Personal    80      720
```

**Calculations:**
- TP = 350 (Work correctly classified)
- FP = 80 (Personal misclassified as work)
- TN = 720 (Personal correctly classified)
- FN = 50 (Work misclassified as personal)

### ğŸ“– **Ã–klid Mesafesi (Euclidean Distance)**

**FormÃ¼l:**
```
d(xâƒ—, yâƒ—) = âˆš(âˆ‘áµ¢â‚Œâ‚â¿ (xáµ¢ - yáµ¢)Â²)
```

**Ä°ngilizce Pratik Ã–rnek:**
```
DokÃ¼man A: [1, 2, 3, 0, 1] (the, cat, sat, on, mat)
DokÃ¼man B: [2, 1, 3, 1, 0] (the, dog, sat, on, floor)

d(A,B) = âˆš((1-2)Â² + (2-1)Â² + (3-3)Â² + (0-1)Â² + (1-0)Â²)
d(A,B) = âˆš(1 + 1 + 0 + 1 + 1) = âˆš4 = 2
```

**TÃ¼rkÃ§e Pratik Ã–rnek:**
```
DokÃ¼man A: [2, 1, 0, 1, 1] (kedi, evde, uyuyor, Ã§ok, mutlu)
DokÃ¼man B: [1, 2, 1, 0, 1] (kedi, bahÃ§ede, oynuyor, Ã§ok, mutlu)

d(A,B) = âˆš((2-1)Â² + (1-2)Â² + (0-1)Â² + (1-0)Â² + (1-1)Â²)
d(A,B) = âˆš(1 + 1 + 1 + 1 + 0) = âˆš4 = 2
```

### ğŸ“– **KosinÃ¼s BenzerliÄŸi (Cosine Similarity)**

**FormÃ¼l:**
```
cos(Î¸) = (xâƒ— Â· yâƒ—) / (||xâƒ—|| Ã— ||yâƒ—||)
```

**Ä°ngilizce Pratik Ã–rnek:**
```
DokÃ¼man A: [1, 2, 3, 0, 1] (the, cat, sat, on, mat)
DokÃ¼man B: [2, 1, 3, 1, 0] (the, dog, sat, on, floor)

A Â· B = 1Ã—2 + 2Ã—1 + 3Ã—3 + 0Ã—1 + 1Ã—0 = 2 + 2 + 9 + 0 + 0 = 13
||A|| = âˆš(1Â² + 2Â² + 3Â² + 0Â² + 1Â²) = âˆš15
||B|| = âˆš(2Â² + 1Â² + 3Â² + 1Â² + 0Â²) = âˆš15

cos(Î¸) = 13 / (âˆš15 Ã— âˆš15) = 13/15 â‰ˆ 0.867
```

**TÃ¼rkÃ§e Pratik Ã–rnek:**
```
DokÃ¼man A: [2, 1, 0, 1, 1] (kedi, evde, uyuyor, Ã§ok, mutlu)
DokÃ¼man B: [1, 2, 1, 0, 1] (kedi, bahÃ§ede, oynuyor, Ã§ok, mutlu)

A Â· B = 2Ã—1 + 1Ã—2 + 0Ã—1 + 1Ã—0 + 1Ã—1 = 2 + 2 + 0 + 0 + 1 = 5
||A|| = âˆš(2Â² + 1Â² + 0Â² + 1Â² + 1Â²) = âˆš7
||B|| = âˆš(1Â² + 2Â² + 1Â² + 0Â² + 1Â²) = âˆš7

cos(Î¸) = 5 / (âˆš7 Ã— âˆš7) = 5/7 â‰ˆ 0.714
```

### âš ï¸ **Farklar**
- **Ã–klid:** Mutlak mesafe, kÃ¼Ã§Ã¼k deÄŸer = benzer
- **KosinÃ¼s:** AÃ§Ä± benzerliÄŸi, bÃ¼yÃ¼k deÄŸer = benzer
- **KosinÃ¼s:** DokÃ¼man uzunluÄŸundan etkilenmez

### ğŸ“Š **KarÅŸÄ±laÅŸtÄ±rmalÄ± Analiz:**

| Ã–zellik | Ä°ngilizce | TÃ¼rkÃ§e |
|---------|-----------|--------|
| **Ã–klid Mesafesi** | 2.0 | 2.0 |
| **KosinÃ¼s BenzerliÄŸi** | 0.867 | 0.714 |
| **DokÃ¼man UzunluÄŸu** | 5 kelime | 5 kelime |
| **Ortak Kelimeler** | 3 (the, sat, on) | 3 (kedi, Ã§ok, mutlu) |
| **Benzerlik Seviyesi** | YÃ¼ksek | Orta | 